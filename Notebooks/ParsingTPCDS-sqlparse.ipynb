{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "from sqlparse.sql import IdentifierList, Identifier\n",
    "from sqlparse.tokens import Keyword, DML\n",
    "from query_representation.utils import *\n",
    "\n",
    "#from moz_sql_parser import parse\n",
    "#from mo_sql_parsing import parse as parse\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDIR = \"./queries/tpcds/all\"\n",
    "#QDIR = \"queries/job/all_job/sqls/all_job\"\n",
    "#QDIR = \"ceb-unique-sqls\"\n",
    "TPCDS_TABLES=\"./queries/tpcds/tpcds.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(TPCDS_TABLES) as f:\n",
    "    tdata = f.readlines()\n",
    "#print(tdata)\n",
    "\n",
    "COLS_TO_TABLES = {}\n",
    "\n",
    "for li, line in enumerate(tdata):\n",
    "    #print(line)\n",
    "    if \"create table\" in line:\n",
    "        #print(line)\n",
    "        vals = line.split(\" \")\n",
    "        #print(\"table: \", vals[2])\n",
    "        table = vals[2].replace(\"\\n\", \"\")\n",
    "        \n",
    "        for i in range(1000):\n",
    "            cline = tdata[li+i]\n",
    "            cline = cline.lstrip()\n",
    "            #print(cline)\n",
    "                \n",
    "            if \"create table\" in cline or \"primary key\" in cline:\n",
    "                continue\n",
    "            vals = cline.split(\" \")\n",
    "            col = vals[0]\n",
    "            \n",
    "            if not (\")\" in col or \"(\" in col or col == \"\"):\n",
    "                if col in COLS_TO_TABLES:\n",
    "                    print(\"?>>???\")\n",
    "                    print(col)\n",
    "                assert col not in COLS_TO_TABLES   \n",
    "                COLS_TO_TABLES[col] = table\n",
    "            \n",
    "            if \")\" in cline and \",\" not in cline:\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(COLS_TO_TABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"catery\" in COLS_TO_TABLES\n",
    "for k in COLS_TO_TABLES:\n",
    "    if \"cat\" in k:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNS = os.listdir(QDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(FNS))\n",
    "print(FNS[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqls = []\n",
    "for fn in FNS:\n",
    "    if \".sql\" not in fn:\n",
    "        continue\n",
    "    if \"all\" in fn:\n",
    "        continue\n",
    "    fn = os.path.join(QDIR, fn)\n",
    "    with open(fn, \"r\") as f:\n",
    "        sql = f.read()\n",
    "    #if \"like\" in sql.lower():\n",
    "        #print(sql)\n",
    "    sql = sql.replace(\"IN\", \"in\")\n",
    "    sql = sql.replace(\"In\", \"in\")\n",
    "    sql = sql.replace(\"ILIKE\", \"like\")\n",
    "    sqls.append(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sqls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for si, sql in enumerate(sqls):\n",
    "#     if \"like\" in sql.lower():\n",
    "#         print(si)\n",
    "#     if \"q91\" in sql.lower():\n",
    "#         print(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sql = sqls[40]\n",
    "sql = sqls[78]\n",
    "# sql = sql.replace(\"IN\", \"in\")\n",
    "# sql = sql.replace(\"In\", \"in\")\n",
    "# sql = sql.replace(\"ILIKE\", \"like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cols_from_tokens(tokens):\n",
    "#     cols = []\n",
    "#     for token in tokens:\n",
    "#         if isinstance(token, sqlparse.sql.Identifier):\n",
    "#             cols.append(str(token.value))\n",
    "#     return cols\n",
    "\n",
    "def get_function_cols(func_token):\n",
    "    cols = []\n",
    "    for param in func_token.get_parameters():\n",
    "        if isinstance(param, sqlparse.sql.Identifier):\n",
    "            cols.append(param.value)\n",
    "    return cols\n",
    "\n",
    "def parse_sql_preds(sql):\n",
    "    def get_table_col(left):\n",
    "        if \".\" in left:\n",
    "            tablename = left[0:left.find(\".\")]\n",
    "            col = left[left.find(\".\")+1:]\n",
    "        else:\n",
    "            tablename = \"X\"\n",
    "            col = left\n",
    "        return tablename, col\n",
    "        \n",
    "    def parse_where(wtokens):\n",
    "        curwhere = {}\n",
    "        for idx, token in enumerate(wtokens):\n",
    "            if \"where\" in str(type(token)).lower():\n",
    "                parse_where(token)\n",
    "                continue\n",
    "            \n",
    "            if isinstance(token, sqlparse.sql.Parenthesis):\n",
    "                assert hasattr(token, \"tokens\")\n",
    "                #parse_recursive(token)\n",
    "                parse_where(token)\n",
    "                continue\n",
    "            \n",
    "            if \"comparison\" in str(type(token)).lower():\n",
    "                #print(token.value)\n",
    "                assert hasattr(token, \"tokens\")\n",
    "                left = None\n",
    "                op = None\n",
    "                right = None\n",
    "                valtypes = []\n",
    "        \n",
    "                for ctoken in token:\n",
    "                    if ctoken.value.strip() == \"\":\n",
    "                        continue\n",
    "                    assert right is None\n",
    "            \n",
    "                    if left is None:\n",
    "                        left = ctoken\n",
    "                        valtypes.append(type(ctoken))\n",
    "                        continue\n",
    "                    if op is None:\n",
    "                        op = ctoken\n",
    "                        continue\n",
    "                        \n",
    "                    if right is None:\n",
    "                        right = ctoken\n",
    "                        continue\n",
    "                    \n",
    "                if isinstance(left, sqlparse.sql.Identifier):\n",
    "                    #left = left.value\n",
    "                    tablename, col = get_table_col(left.value)  \n",
    "                elif isinstance(left, sqlparse.sql.Function):\n",
    "                    tablename = \"function\"\n",
    "                    allcols = get_function_cols(left)\n",
    "                    #print(\"Num function columns: \", len(allcols))\n",
    "                    col = \",\".join(allcols)\n",
    "                elif isinstance(left, sqlparse.sql.Parenthesis):\n",
    "                    assert hasattr(left, \"tokens\")\n",
    "                    parse_where(left)\n",
    "                    tablename = \"function\"\n",
    "                    col = \"function\"\n",
    "                elif isinstance(left, sqlparse.sql.Operation):\n",
    "                    tablename = \"function\"\n",
    "                    col = \"function\"\n",
    "                else:\n",
    "                    # FIXME: handle these better\n",
    "                    if isinstance(right, sqlparse.sql.Identifier):\n",
    "                        tablename, col = get_table_col(right.value)\n",
    "                        tmp1 = right\n",
    "                        right = left\n",
    "                        left = tmp1\n",
    "                        # FIXME: need to change other stuff, like operator type too\n",
    "                    else:\n",
    "                        assert False\n",
    "                    \n",
    "                if \"catery\" in col:\n",
    "                    col = col.replace(\"catery\", \"category\")\n",
    "                    \n",
    "                if isinstance(right, sqlparse.sql.Identifier):\n",
    "                    #print(\"Join?\")\n",
    "                    continue\n",
    "                \n",
    "                if isinstance(right, sqlparse.sql.Parenthesis):\n",
    "                    assert hasattr(right, \"tokens\")\n",
    "                    parse_recursive(right)\n",
    "                \n",
    "                # TODO: need a better way to find joins\n",
    "                if not \"token\" in str(type(right)).lower():\n",
    "                    if not \"select\" in right.value and \\\n",
    "                        op.value == \"=\" and \\\n",
    "                        \".\" in right.value:\n",
    "                        # potential join\n",
    "                        continue\n",
    "                \n",
    "                op = op.value.lower()\n",
    "                right = right.value\n",
    "                op = op.lower()\n",
    "                \n",
    "                allpreddata[\"left\"].append(left.value)\n",
    "                allpreddata[\"table\"].append(tablename)   \n",
    "                allpreddata[\"col\"].append(col)\n",
    "                allpreddata[\"op\"].append(op)\n",
    "                allpreddata[\"val\"].append(str(right))\n",
    "            \n",
    "            #### TODO:\n",
    "            if \"between\" in str(token):\n",
    "                left = wtokens[idx-2]\n",
    "                \n",
    "                if isinstance(left, sqlparse.sql.Identifier):\n",
    "                    tablename, col = get_table_col(left.value)\n",
    "                    #print(tablename, col)\n",
    "                elif isinstance(left, sqlparse.sql.Parenthesis):\n",
    "                    tablename = \"betweenX\"\n",
    "                    col = \"func\"\n",
    "                else:\n",
    "                    tablename = \"betweenX\"\n",
    "                    col = \"unk\"\n",
    "                    #assert False\n",
    "                \n",
    "                allpreddata[\"left\"].append(left.value)\n",
    "                allpreddata[\"table\"].append(tablename)\n",
    "                allpreddata[\"col\"].append(col)\n",
    "                allpreddata[\"op\"].append(\"between\")\n",
    "                allpreddata[\"val\"].append(-1)\n",
    "                \n",
    "#                 left = wtokens[idx+2]\n",
    "#                 right = wtokens[idx+6]\n",
    "# FIXME: general alternative\n",
    "#                 for ci in range(-2,7,1):\n",
    "#                     print(ci)\n",
    "#                     print(wtokens[idx+ci])\n",
    "            \n",
    "    def parse_recursive(tokens):\n",
    "        for idx, token in enumerate(tokens):\n",
    "            if hasattr(token, \"tokens\"):\n",
    "                parse_recursive(token)\n",
    "            \n",
    "            # original code\n",
    "            if \"where\" in str(type(token)).lower():\n",
    "                parse_where(token)\n",
    "            \n",
    "            if token.value.lower() in [\"having\", \"group by\"]:\n",
    "                # TODO: parse\n",
    "                index, token2 = tokens.token_next(idx)\n",
    "        \n",
    "    allpreddata = defaultdict(list)\n",
    "    allpreds = []\n",
    "    \n",
    "    if sql.strip() == \"\":\n",
    "        return pd.DataFrame(allpreddata)\n",
    "\n",
    "    parsed = sqlparse.parse(sql)\n",
    "    assert len(parsed) == 1\n",
    "    parse_recursive(parsed[0])\n",
    "    #return allpreds\n",
    "    df = pd.DataFrame(allpreddata)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = parse_sql_preds(sql)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print(preds)\n",
    "# p0 = preds[0]\n",
    "\n",
    "# mergedd = {}\n",
    "# for p in preds:\n",
    "#     for table,colvals in p.items():\n",
    "#         if table not in mergedd:\n",
    "#             mergedd[table] = {}\n",
    "#         for col,opvals in colvals.items():\n",
    "#             if col not in mergedd[table]:\n",
    "#                 mergedd[table][col] = {}\n",
    "#             for op,vals in opvals.items():\n",
    "#                 if op not in mergedd[table][col]:\n",
    "#                     mergedd[table][col][op] = set()\n",
    "#                 for cval in vals:\n",
    "#                     mergedd[table][col][op].add(cval)\n",
    "                \n",
    "\n",
    "# #print(mergedd)\n",
    "# for table,vals in mergedd.items():\n",
    "#     print(table)\n",
    "#     print(vals)\n",
    "#     print(\"***********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-clock",
   "metadata": {},
   "source": [
    "# Q87 very hard to parse;\n",
    "### TODO: maybe queries w/ multiple subqueries, we just try to get each select block and parse them individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql4 = \"\"\"-- q87\n",
    "\n",
    "\n",
    "select count(*) \n",
    "from (select distinct c_last_name, c_first_name, d_date\n",
    "       from store_sales, date_dim, customer  -- skan_memo_stash_87\n",
    "       where store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
    "         and store_sales.ss_customer_sk = customer.c_customer_sk\n",
    "         and d_month_seq between 1212 and 1212+11)\n",
    "       except\n",
    "      (select distinct c_last_name, c_first_name, d_date\n",
    "       from catalog_sales, date_dim, customer\n",
    "       where catalog_sales.cs_sold_date_sk = date_dim.d_date_sk\n",
    "         and catalog_sales.cs_bill_customer_sk = customer.c_customer_sk\n",
    "         and d_month_seq between 1212 and 1212+11)\n",
    "       except\n",
    "      (select distinct c_last_name, c_first_name, d_date\n",
    "       from web_sales, date_dim, customer\n",
    "       where web_sales.ws_sold_date_sk = date_dim.d_date_sk\n",
    "         and web_sales.ws_bill_customer_sk = customer.c_customer_sk\n",
    "         and d_month_seq between 1212 and 1212+11)\n",
    "cool_cust\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# sql4 = \"\"\"-- q87\n",
    "\n",
    "\n",
    "# select count(*) \n",
    "# from ((select distinct c_last_name, c_first_name, d_date\n",
    "#        from store_sales, date_dim, customer  -- skan_memo_stash_87\n",
    "#        where store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
    "#          and store_sales.ss_customer_sk = customer.c_customer_sk\n",
    "#          and d_month_seq between 1212 and 1212+11)\n",
    "#        except\n",
    "#       (select distinct c_last_name, c_first_name, d_date\n",
    "#        from catalog_sales, date_dim, customer\n",
    "#        where catalog_sales.cs_sold_date_sk = date_dim.d_date_sk\n",
    "#          and catalog_sales.cs_bill_customer_sk = customer.c_customer_sk\n",
    "#          and d_month_seq between 1212 and 1212+11))\n",
    "# cool_cust\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# sql4 = \"\"\"-- q87\n",
    "\n",
    "\n",
    "# select count(*) \n",
    "# from (select distinct c_last_name, c_first_name, d_date\n",
    "#        from store_sales, date_dim, customer  -- skan_memo_stash_87\n",
    "#        where store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
    "#          and store_sales.ss_customer_sk = customer.c_customer_sk\n",
    "#          and d_month_seq between 1212 and 1212+11)\n",
    "# cool_cust\n",
    "# \"\"\"\n",
    "parse_sql_preds(sql4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-selling",
   "metadata": {},
   "source": [
    "# Fix for Q40, change order of from tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql3 = \"\"\"select top 100 \n",
    "   w_state\n",
    "  ,i_item_id\n",
    "  ,sum(case when (cast(d_date as date) < cast ('1998-04-08' as date)) \n",
    "        then cs_sales_price - coalesce(cr_refunded_cash,0) else 0 end) as sales_before\n",
    "  ,sum(case when (cast(d_date as date) >= cast ('1998-04-08' as date)) \n",
    "        then cs_sales_price - coalesce(cr_refunded_cash,0) else 0 end) as sales_after\n",
    " from\n",
    "   warehouse  -- skan_memo_stash_40\n",
    "  ,item\n",
    "  ,date_dim\n",
    "  ,catalog_sales left outer join catalog_returns on\n",
    "       (cs_order_number = cr_order_number \n",
    "        and cs_item_sk = cr_item_sk)\n",
    "\n",
    " where\n",
    "     i_current_price between 0.99 and 1.49\n",
    " and i_item_sk          = cs_item_sk\n",
    " and cs_warehouse_sk    = w_warehouse_sk \n",
    " and cs_sold_date_sk    = d_date_sk\n",
    " and d_date >= dateadd(dd, -30, '1998-04-08')\n",
    " and d_date <= dateadd(dd, 30, '1998-04-08') \n",
    " group by\n",
    "    w_state,i_item_id\n",
    " order by w_state,i_item_id\"\"\"\n",
    "parse_sql_preds(sql3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-election",
   "metadata": {},
   "source": [
    "# replace intersect by union seems to work for now;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2 = \"\"\"select top 100 count(*) from (\n",
    "    select distinct c_last_name, c_first_name, d_date\n",
    "    from store_sales, date_dim, customer -- skan_memo_stash_38\n",
    "          where store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
    "      and store_sales.ss_customer_sk = customer.c_customer_sk\n",
    "      and d_month_seq between 1212 and 1212 + 11) test\"\"\"\n",
    "sql2 = \"\"\"select top 100 count(*) from (\n",
    "    select distinct c_last_name, c_first_name, d_date\n",
    "    from store_sales, date_dim, customer -- skan_memo_stash_38\n",
    "          where store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
    "      and store_sales.ss_customer_sk = customer.c_customer_sk\n",
    "      and d_month_seq between 1212 and 1212 + 11\n",
    "  union\n",
    "    select distinct c_last_name, c_first_name, d_date\n",
    "    from catalog_sales, date_dim, customer\n",
    "          where catalog_sales.cs_sold_date_sk = date_dim.d_date_sk\n",
    "      and catalog_sales.cs_bill_customer_sk = customer.c_customer_sk\n",
    "      and d_month_seq between 1212 and 1212 + 11) tmp\"\"\"\n",
    "parse_sql_preds(sql2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_cont_vals(ops, vals, col_names):\n",
    "    \n",
    "#     # values to return: one for each column\n",
    "#     ## type: range, lt, gt\n",
    "#     ## dtype:\n",
    "#     ## range: None, or actual value\n",
    "#     ret = {}\n",
    "#     cont_idxs = []\n",
    "    \n",
    "#     cur_comb_op = \"\"\n",
    "#     for oi, op in enumerate(ops):\n",
    "#         if op == \"And\" or op == \"Or\":\n",
    "#             cur_comb_op = op\n",
    "        \n",
    "#         if \">\" in op or \"<\" in op:\n",
    "#             cont_idxs.append(oi)\n",
    "#             col = col_names[oi]\n",
    "            \n",
    "#             col = col_names[oi]\n",
    "# #             val = vals[oi]\n",
    "# #             if isinstance(val, list):\n",
    "# #                 if len(val) == 0:\n",
    "# #                     continue\n",
    "# #                 assert len(val) == 1\n",
    "# #                 val = val[0]\n",
    "            \n",
    "# #             val = val.replace(\"@\", \"\")\n",
    "# #             val = val.replace(\"\\\"\", \"\")\n",
    "# #             val = val.replace(\"\\'\", \"\")\n",
    "# #             dtype, pval = get_cont_dtype(val)\n",
    "            \n",
    "#             if col not in ret:\n",
    "#                 ret[col] = {}\n",
    "#             ret[col][\"dtype\"] = dtype\n",
    "#             if pval is not None and \">\" in op:\n",
    "#                 ret[col][\"gt\"] = pval\n",
    "#             elif pval is not None and \"<\" in op:\n",
    "#                 ret[col][\"lt\"] = pval\n",
    "            \n",
    "#             ret[col][\"comb_op\"] = cur_comb_op\n",
    "    \n",
    "#     for col in ret:\n",
    "#         if \"comb_op\" in ret[col] and ret[col][\"comb_op\"] == \"Or\":\n",
    "#             ctype = \"discont\"\n",
    "        \n",
    "#         elif \"lt\" in ret[col] and \"gt\" in ret[col]:\n",
    "#             ctype = \"range\"\n",
    "#             #assert ret[col][\"comb_op\"] == \"And\"\n",
    "#             if \"comb_op\" not in ret[col]:\n",
    "#                 print(vals)\n",
    "#                 print(ops)\n",
    "\n",
    "#             elif ret[col][\"comb_op\"] != \"And\":\n",
    "#                 print(vals)\n",
    "#                 print(ops)\n",
    "            \n",
    "#         elif \"lt\" in ret[col]:\n",
    "#             ctype = \"lt\"\n",
    "#         elif \"gt\" in ret[col]:\n",
    "#             ctype = \"gt\"\n",
    "#         else:\n",
    "#             ctype = \"other\"\n",
    "        \n",
    "#         if \"lt\" in ret[col] and \"gt\" in ret[col]:\n",
    "#             crange = ret[col][\"lt\"] - ret[col][\"gt\"]\n",
    "#             if crange < 0:\n",
    "#                 assert ret[col][\"comb_op\"] == \"Or\"\n",
    "#             else:\n",
    "#                 ret[col][\"range\"] = crange\n",
    "        \n",
    "#         ret[col][\"cont_type\"] = ctype\n",
    "\n",
    "#     return ret\n",
    "\n",
    "def is_num(val):\n",
    "    try:\n",
    "        float(val)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def is_a_date(val):\n",
    "    \n",
    "    val = val.replace(\"'\", \"\")\n",
    "    if \"date\" in val:\n",
    "        return True\n",
    "    \n",
    "    elif is_num(val):\n",
    "        num = float(val)\n",
    "        if num > 1900 and num < 2010:\n",
    "            return True\n",
    "        \n",
    "    elif \"+\" in val:\n",
    "        cvals = val[0:val.find(\"+\")]\n",
    "        if is_num(cvals):\n",
    "            num = float(cvals)\n",
    "            if num > 1900 and num < 2010:\n",
    "                return True\n",
    "            \n",
    "    elif \"-\" in val:\n",
    "#         if val.count(\"-\") == 2:\n",
    "#             print(val)\n",
    "        cvals = val[0:val.find(\"-\")]\n",
    "        if is_num(cvals):\n",
    "            num = float(cvals)\n",
    "            if num > 1900 and num < 2010:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "import enchant\n",
    "enchantD = enchant.Dict(\"en_US\")\n",
    "import re\n",
    "#import nltk\n",
    "#nltk.download('words')\n",
    "#from nltk.corpus import words as nltkwords\n",
    "\n",
    "URLPAT = \"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\n",
    "def get_like_type(op, vals):\n",
    "    likekind = \"-1\"\n",
    "    likedtype = \"-1\"\n",
    "    \n",
    "    if \"like\" not in op.lower():\n",
    "        return likedtype, likekind\n",
    "    \n",
    "    vals = vals.replace(\"'\", \"\")\n",
    "    if vals.count(\"%\") == 0:\n",
    "        likekind = \"no%\"\n",
    "    elif vals.count(\"%\") == 2:\n",
    "        likekind = \"contains\"\n",
    "    elif vals.count(\"%\") == 1:\n",
    "        if vals[0] == \"%\":\n",
    "            likekind = \"ends\"\n",
    "        elif vals[-1] == \"%\":\n",
    "            likekind = \"starts\"\n",
    "        else:\n",
    "            assert False\n",
    "    else:\n",
    "        nlikes = vals.count(\"%\")\n",
    "        likekind = str(nlikes) + \"%\"\n",
    "    \n",
    "    opval = vals.replace(\"%\", \"\")\n",
    "    \n",
    "    if is_num(opval):\n",
    "        likedtype = \"num\"\n",
    "    elif len(opval) <= 2:\n",
    "        likedtype = \"short\"\n",
    "    elif opval[0] == \".\":\n",
    "        likedtype = \"extension\"\n",
    "    elif re.match(URLPAT, opval) is not None:\n",
    "        if (\"cosmos\" in opval or \"adl\" in opval) and \"/\" in opval:\n",
    "            likedtype = \"path\"\n",
    "        else:\n",
    "            likedtype = \"url\"\n",
    "            \n",
    "    elif opval.count(\"/\") >= 2 or opval.count(\"\\\\\") >= 2:\n",
    "        likedtype = \"path\"\n",
    "        \n",
    "    elif enchantD.check(opval):\n",
    "        likedtype = \"word\"\n",
    "\n",
    "    elif opval.count(\"-\") >= 2 or \\\n",
    "            opval.count(\":\") >= 2:\n",
    "        likedtype = \"serial\"\n",
    "    elif \"-\" in opval or \"_\" in opval or \" \" in opval or \",\" in opval or \":\" in opval:\n",
    "        if \"-\" in opval:       \n",
    "            allvals = opval.split(\"-\")\n",
    "        elif \"_\" in opval:\n",
    "            allvals = opval.split(\"_\")\n",
    "        elif \" \" in opval:\n",
    "            allvals = opval.split(\" \")\n",
    "        elif \",\" in opval:\n",
    "            allvals = opval.split(\",\")\n",
    "        elif \":\" in opval:\n",
    "            allvals = opval.split(\":\")\n",
    "            \n",
    "        validwords = 0\n",
    "        for v1 in allvals:\n",
    "            if v1 == \"\":\n",
    "                continue\n",
    "            if enchantD.check(v1):\n",
    "                validwords += 1\n",
    "        if validwords >= 2:\n",
    "            likedtype = \"words\"\n",
    "    elif \"0x\" in opval:\n",
    "        likedtype = \"hex\"\n",
    "    else:\n",
    "        validwords = 0\n",
    "        start = 0\n",
    "        prevstart = 0\n",
    "        for oi,_ in enumerate(opval):\n",
    "            cword = opval[start:oi+1]\n",
    "            pword = opval[prevstart:oi+1]\n",
    "            if enchantD.check(cword) and len(cword) >= 3:\n",
    "                validwords += 1\n",
    "                prevstart = start\n",
    "                start = oi+1\n",
    "            \n",
    "            # extend previous word\n",
    "            elif enchantD.check(pword) and len(pword) >= 3:\n",
    "                start = oi+1\n",
    "                \n",
    "        if validwords >= 2:\n",
    "            likedtype = \"words\"\n",
    "        else:\n",
    "            likedtype = \"unknown\"\n",
    "    \n",
    "    return likedtype, likekind\n",
    "\n",
    "def get_discrete_type(op, vals):\n",
    "    dtype = \"-1\"\n",
    "    dkind = \"-1\"\n",
    "    #op = op.lower()\n",
    "    assert op != \"\"\n",
    "    if op in [\"=\", \"!=\", \"<>\", \"in\"]:\n",
    "        if op == \"=\":\n",
    "            dkind = \"eq\"\n",
    "        elif op in [\"!=\", \"<>\"]:\n",
    "            print(\"neq!!\")\n",
    "            dkind = \"neq\"\n",
    "        else:\n",
    "            dkind = \"in\"\n",
    "            \n",
    "        if \"select\" in vals:\n",
    "            dtype = \"sql\"\n",
    "        else: \n",
    "            if \"(\" in vals:\n",
    "                vals = vals.replace(\"(\", \"\")\n",
    "                vals = vals.replace(\")\", \"\")\n",
    "                vals = vals.split(\",\")\n",
    "                vals = vals[0]\n",
    "            \n",
    "            if is_a_date(vals):\n",
    "                dtype = \"date\"\n",
    "            elif is_num(vals):\n",
    "                dtype = \"num\"\n",
    "            else:\n",
    "                dtype = \"string\"\n",
    "                \n",
    "    \n",
    "        \n",
    "    return dtype, dkind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "allinpdata = defaultdict(list)\n",
    "allpreds = []\n",
    "\n",
    "for si, sql in enumerate(sqls):\n",
    "    # empty sqls\n",
    "    if si in [78, 85]:\n",
    "        continue\n",
    "    print(\"Query: \", si)\n",
    "    alldfdata = defaultdict(list)\n",
    "    \n",
    "    numwheres = sql.lower().count(\"where\")\n",
    "    preds = parse_sql_preds(sql)\n",
    "    olen = len(preds)\n",
    "    preds = preds.drop_duplicates()\n",
    "    nlen = len(preds)\n",
    "    \n",
    "    preds[\"qid\"] = si\n",
    "    \n",
    "    curcols = []\n",
    "    curops = []\n",
    "    for idx, row in preds.iterrows():\n",
    "        curcols.append(row[\"col\"])\n",
    "        curops.append(row[\"op\"])\n",
    "        \n",
    "        if row[\"col\"] in COLS_TO_TABLES:\n",
    "            tab = COLS_TO_TABLES[row[\"col\"]]\n",
    "            col = row[\"col\"]\n",
    "        elif row[\"table\"] != \"X\":\n",
    "            tab = row[\"table\"]\n",
    "            col = row[\"col\"]\n",
    "        else:\n",
    "            tab = row[\"table\"]\n",
    "        \n",
    "        op = row[\"op\"]\n",
    "        alldfdata[\"table\"].append(tab)\n",
    "        alldfdata[\"col\"].append(col)\n",
    "        dtype,dkind = get_discrete_type(op, row[\"val\"])\n",
    "        alldfdata[\"dtype\"].append(dtype)\n",
    "        alldfdata[\"dkind\"].append(dkind)\n",
    "        \n",
    "        likedtype,likekind = get_like_type(op, row[\"val\"])\n",
    "        alldfdata[\"likedtype\"].append(likedtype)\n",
    "        alldfdata[\"likekind\"].append(likekind)\n",
    "        \n",
    "        if op in [\"=\", \"!=\"]:\n",
    "            alldfdata[\"discrete_ops\"].append(1)\n",
    "            alldfdata[\"in_ops\"].append(0)\n",
    "            allinpdata[\"col\"].append(row[\"col\"])\n",
    "            allinpdata[\"val\"].append(row[\"val\"])\n",
    "            \n",
    "        elif op == \"in\":\n",
    "            alldfdata[\"discrete_ops\"].append(1)\n",
    "            alldfdata[\"in_ops\"].append(1)\n",
    "            if \"select\" in row[\"val\"]:\n",
    "                pass\n",
    "            else:\n",
    "                cvals = row[\"val\"]\n",
    "                cvals = cvals.replace(\"(\", \"\")\n",
    "                cvals = cvals.replace(\")\", \"\")\n",
    "                cvals = cvals.split(\",\")\n",
    "                for cval in cvals:\n",
    "                    allinpdata[\"col\"].append(row[\"col\"])\n",
    "                    allinpdata[\"val\"].append(cval)\n",
    "        else:\n",
    "            alldfdata[\"discrete_ops\"].append(0)\n",
    "            alldfdata[\"in_ops\"].append(0)\n",
    "\n",
    "        if op in [\">\", \"<\", \">=\", \"<=\", \"between\"]:\n",
    "            alldfdata[\"cont_ops\"].append(1)\n",
    "        else:\n",
    "            alldfdata[\"cont_ops\"].append(0)\n",
    "\n",
    "        if op == \"like\":\n",
    "            alldfdata[\"like_ops\"].append(1)\n",
    "        else:\n",
    "            alldfdata[\"like_ops\"].append(0)\n",
    "    \n",
    "    if len(preds) == 0:\n",
    "        continue\n",
    "    \n",
    "    tmpdf = pd.DataFrame(alldfdata)\n",
    "    assert len(tmpdf) == len(preds)\n",
    "#     preds[\"like_ops\"] = tmpdf[\"like_ops\"]\n",
    "#     preds[\"cont_ops\"] = tmpdf[\"cont_ops\"]\n",
    "#     preds[\"discrete_ops\"] = tmpdf[\"discrete_ops\"]\n",
    "#     preds[\"in_ops\"] = tmpdf[\"in_ops\"]\n",
    "#     preds[\"dtype\"] = tmpdf[\"dtype\"]\n",
    "#     preds[\"dkind\"] = tmpdf[\"dkind\"]\n",
    "#     preds[\"likekind\"] = tmpdf[\"likekind\"]\n",
    "    for k in tmpdf.keys():\n",
    "        preds[k] = tmpdf[k]\n",
    "        \n",
    "    allpreds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in alldfdata.items():\n",
    "#     print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(allpreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.keys())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"ceb_queries_df.csv\", index=False)\n",
    "df.to_csv(\"tpcds_queries_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv(\"job_queries_df.csv\")\n",
    "# df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(df[[\"like_ops\", \"discrete_ops\", \"cont_ops\", \"in_ops\"]].\\\n",
    "     describe(percentiles=[0.9,0.99]).reset_index().to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(set(df[\"dkind\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "discdf = df[df.discrete_ops == 1]\n",
    "discdf.groupby([\"dkind\"]).count()[\"discrete_ops\"].reset_index().sort_values(by=\"discrete_ops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "discdf.groupby([\"dtype\"]).count()[\"discrete_ops\"].reset_index().sort_values(by=\"discrete_ops\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = df[df.like_ops == 1]\n",
    "ldf.groupby(\"likedtype\").count()[\"discrete_ops\"].reset_index().sort_values(by=\"discrete_ops\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf.groupby(\"likekind\").count()[\"discrete_ops\"].reset_index().sort_values(by=\"discrete_ops\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpdata = pd.DataFrame(allinpdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(inpdata))\n",
    "inpdata = inpdata.drop_duplicates()\n",
    "inpdata = inpdata[inpdata[\"col\"] != \"function\"]\n",
    "print(len(inpdata))\n",
    "print(len(set(inpdata[\"col\"])))\n",
    "print(set(inpdata[\"col\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-dragon",
   "metadata": {},
   "source": [
    "# unique values seen in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "inpdata.groupby(\"col\").count().reset_index()[\"val\"].describe(percentiles=[0.75,0.9,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = df[df[\"col\"] == \"function\"]\n",
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf = pd.concat(allpreds)\n",
    "pdf = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf = pdf[pdf.col.isin(COLS_TO_TABLES)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-camcorder",
   "metadata": {},
   "source": [
    "# TODO: need equivalent version for SCOPE;\n",
    "## TODO: more important is #columns per input in a query; for this we need column --> table mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.groupby([\"col\", \"qid\"])[\"op\"].nunique().reset_index().sort_values(by=\"op\", ascending=False)[\"op\"].describe(\n",
    "                                    percentiles=[0.75,0.90,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in set(pdf[\"col\"]):\n",
    "    if not col in COLS_TO_TABLES:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2 = pdf[pdf.table != \"X\"]\n",
    "print(set(pdf2[\"table\"]))\n",
    "print(len(set(pdf2[\"table\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.groupby([\"table\", \"qid\"])[\"col\"].nunique().reset_index().sort_values(by=\"col\", ascending=False)[\"col\"].describe(\n",
    "                                    percentiles=[0.75,0.90,0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pdf2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2.groupby([\"table\", \"qid\"])[\"col\"].nunique().reset_index().sort_values(by=\"col\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-flashing",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "#### More columns being filtered (tails) on BUT within a column ---- fewer categorical values being hit\n",
    "#### discrete_type: num vs string\n",
    "#### cont dtypes: range vs leq vs geq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.groupby(\"op\").count()[\"qid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-paintball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-placement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
