{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "from query_representation.utils import *\n",
    "\n",
    "from torch.utils import data\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-separate",
   "metadata": {},
   "source": [
    "# Setup file paths / Download query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def make_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "def eval_alg(alg, eval_funcs, qreps, samples_type, result_dir=\"./results/\"):\n",
    "    '''\n",
    "    '''\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "    print(\"start eval alg\")\n",
    "    if not isinstance(alg, list):\n",
    "        ests = alg.test(qreps)\n",
    "        alg_name = alg.__str__()\n",
    "        exp_name = alg.get_exp_name()\n",
    "    else:\n",
    "        ests = alg\n",
    "        alg_name = \"Estimates\"\n",
    "        exp_name = \"test\"\n",
    "    \n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    \n",
    "    print(\"before eval funcs\")\n",
    "    for efunc in eval_funcs:\n",
    "        rdir = None\n",
    "        if result_dir is not None:\n",
    "            rdir = os.path.join(result_dir, exp_name)\n",
    "            make_dir(rdir)\n",
    "\n",
    "        errors = efunc.eval(qreps, ests, samples_type=samples_type,\n",
    "                result_dir=rdir,\n",
    "                num_processes = -1,\n",
    "                alg_name = alg_name,\n",
    "                use_wandb=0,\n",
    "                user = \"ceb\",\n",
    "                db_name = \"imdb\",\n",
    "                db_host = \"localhost\",\n",
    "                password = \"password\",\n",
    "                port = 5432\n",
    "                )\n",
    "\n",
    "        print(\"{}, {}, #samples: {}, {}: mean: {}, median: {}, 99p: {}\"\\\n",
    "                .format(samples_type, alg_name, len(errors),\n",
    "                    efunc.__str__(),\n",
    "                    np.round(np.mean(errors),3),\n",
    "                    np.round(np.median(errors),3),\n",
    "                    np.round(np.percentile(errors,99),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TESTDIR = os.path.join(os.path.join(\"..\", \"queries\"), \"imdb-unique-plans\")\n",
    "#RESULTDIR = os.path.join(\"..\", \"results\")\n",
    "#make_dir(RESULTDIR)\n",
    "\n",
    "MAXCARD = 150001000000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-collaboration",
   "metadata": {},
   "source": [
    "# Query loading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrep(fn):\n",
    "    assert \".pkl\" in fn\n",
    "    try:\n",
    "        with open(fn, \"rb\") as f:\n",
    "            query = pickle.load(f)\n",
    "    except:\n",
    "        print(fn + \" failed to load...\")\n",
    "        exit(-1)\n",
    "\n",
    "    query[\"subset_graph\"] = \\\n",
    "            nx.OrderedDiGraph(json_graph.adjacency_graph(query[\"subset_graph\"]))\n",
    "    query[\"join_graph\"] = json_graph.adjacency_graph(query[\"join_graph\"])\n",
    "    if \"subset_graph_paths\" in query:\n",
    "        query[\"subset_graph_paths\"] = \\\n",
    "                nx.OrderedDiGraph(json_graph.adjacency_graph(query[\"subset_graph_paths\"]))\n",
    "\n",
    "    return query\n",
    "\n",
    "def load_qdata(fns):\n",
    "    qreps = []\n",
    "    for qfn in fns:\n",
    "        qrep = load_qrep(qfn)\n",
    "        # TODO: can do checks like no queries with zero cardinalities etc.\n",
    "        qreps.append(qrep)\n",
    "        template_name = os.path.basename(os.path.dirname(qfn))\n",
    "        qrep[\"name\"] = os.path.basename(qfn)\n",
    "        qrep[\"template_name\"] = template_name\n",
    "        qrep[\"workload\"] = \"\"\n",
    "    return qreps\n",
    "\n",
    "def get_query_fns(basedir, template_fraction=1.0, sel_templates=None):\n",
    "    fns = []\n",
    "    tmpnames = list(glob.glob(os.path.join(basedir, \"*\")))\n",
    "    print(tmpnames)\n",
    "    assert template_fraction <= 1.0\n",
    "    \n",
    "    if sel_templates == None:\n",
    "        sel_templates = \"all\"\n",
    "    \n",
    "    for qi,qdir in enumerate(tmpnames):\n",
    "        if os.path.isfile(qdir):\n",
    "            print(qdir)\n",
    "            continue\n",
    "        template_name = os.path.basename(qdir)\n",
    "        \n",
    "        if \"no7\" in sel_templates and template_name == \"7a\":\n",
    "            continue\n",
    "            \n",
    "        if \"all\" not in sel_templates and template_name not in sel_templates:\n",
    "            continue\n",
    "        \n",
    "        # let's first select all the qfns we are going to load\n",
    "        qfns = list(glob.glob(os.path.join(qdir, \"*.pkl\")))\n",
    "        qfns.sort()\n",
    "        num_samples = max(int(len(qfns)*template_fraction), 1)\n",
    "        random.seed(1234)\n",
    "        qfns = random.sample(qfns, num_samples)\n",
    "        fns += qfns\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-waterproof",
   "metadata": {},
   "source": [
    "# Evaluation helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-omaha",
   "metadata": {},
   "source": [
    "# Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDIR = \"imdb-unique-plans\"\n",
    "#QDIR = \"job2\"\n",
    "#TEMPLATES = \"all\"\n",
    "TEMPLATES = \"2b\"\n",
    "TEST = \"2a\"\n",
    "#TEMPLATES = \"all-no7\"\n",
    "\n",
    "TRAINDIR = os.path.join(os.path.join(\"/flash1/pari/MyCEB\", \"queries\"), QDIR)\n",
    "RTDIRS = [\"/flash1/pari/MyCEB/runtime_plans/pg\"]\n",
    "qfns = get_query_fns(TRAINDIR, template_fraction = 1.0, sel_templates=TEMPLATES)\n",
    "tqfns = get_query_fns(TRAINDIR, template_fraction = 1.0, sel_templates=TEST)\n",
    "\n",
    "# TRAINDIR = os.path.join(os.path.join(\"/flash1/pari/MyCEB\", \"queries\"), \"job\")\n",
    "#RTDIRS = [\"/flash1/pari/MyCEB/runtime_plans/JOB/\"]\n",
    "# qfns = get_query_fns(TRAINDIR, template_fraction = 1.0, sel_templates=None)\n",
    "\n",
    "print(len(qfns))\n",
    "qdata = load_qdata(qfns)\n",
    "tqdata = load_qdata(tqfns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdfs = []\n",
    "\n",
    "for RTDIR in RTDIRS:\n",
    "    rdirs = os.listdir(RTDIR)\n",
    "    for rd in rdirs:\n",
    "        rtfn = os.path.join(RTDIR, rd, \"Runtimes.csv\")\n",
    "        if os.path.exists(rtfn):\n",
    "            rtdfs.append(pd.read_csv(rtfn))\n",
    "            \n",
    "rtdf = pd.concat(rtdfs)\n",
    "print(\"Num RTs: \", len(rtdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33948ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy\n",
    "\n",
    "subplan_data = defaultdict(list)\n",
    "\n",
    "rowkeys = set()\n",
    "for qi, qrep in enumerate(qdata):\n",
    "    for node in qrep[\"subset_graph\"].nodes():\n",
    "#         if len(node) == 1:\n",
    "#             continue\n",
    "        rowkeys.add(node)\n",
    "        \n",
    "rowkeys = list(rowkeys)\n",
    "rowkeys.sort()\n",
    "rowidxs = {rk:ri for ri,rk in enumerate(rowkeys)}\n",
    "\n",
    "mat = np.zeros((len(rowidxs), len(qdata)))\n",
    "\n",
    "for qi, qrep in enumerate(qdata):\n",
    "    for node in qrep[\"subset_graph\"].nodes():\n",
    "        if node not in rowidxs:\n",
    "            continue\n",
    "        truec = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"actual\"]\n",
    "        if truec >= MAXCARD:\n",
    "            truec = 0.0\n",
    "            \n",
    "        mat[rowidxs[node], qi] = truec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75595ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_plandata(qdata, rtdf):\n",
    "    rowkeys = set()\n",
    "    rtdata = []\n",
    "    for qi, qrep in enumerate(qdata):\n",
    "        if qrep[\"name\"] not in rtdf[\"qname\"].values:\n",
    "            continue\n",
    "        rtdata.append(qrep)\n",
    "    \n",
    "    rowkeys = set()\n",
    "    for qi, qrep in enumerate(rtdata):\n",
    "        for node in qrep[\"subset_graph\"].nodes():\n",
    "            rowkeys.add(node)\n",
    "    rowkeys = list(rowkeys)\n",
    "    rowkeys.sort()\n",
    "    rowidxs = {rk:ri for ri,rk in enumerate(rowkeys)}\n",
    "    \n",
    "    mat = np.zeros((len(rowidxs), len(rtdata)))\n",
    "    planmat = np.zeros((len(rowidxs), len(rtdata)))\n",
    "    subplan_masks = []\n",
    "    \n",
    "    for qi, qrep in enumerate(rtdata):\n",
    "        tmp = rtdf[rtdf[\"qname\"] == qrep[\"name\"]]\n",
    "        exp = tmp[\"exp_analyze\"].values[0]\n",
    "        try:\n",
    "            exp = eval(exp)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        G = explain_to_nx(exp)\n",
    "        seen_subplans = [ndata[\"aliases\"] for n,ndata in G.nodes(data=True)]\n",
    "        subplan_masks.append(seen_subplans)\n",
    "        \n",
    "        for node in qrep[\"subset_graph\"].nodes():\n",
    "            if node not in rowidxs:\n",
    "                continue\n",
    "            truec = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"actual\"]\n",
    "            mat[rowidxs[node], qi] = truec\n",
    "            \n",
    "            if list(node) in seen_subplans:\n",
    "                planmat[rowidxs[node], qi] = truec\n",
    "    \n",
    "    return mat, planmat, rtdata, subplan_masks, rowidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1bd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, S, Q = np.linalg.svd(mat, full_matrices=False)\n",
    "print(mat.shape)\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf9163",
   "metadata": {},
   "outputs": [],
   "source": [
    "S.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(S, 90), np.percentile(P, 90), np.percentile(Q, 90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(S, 90) - np.max(mat))\n",
    "print(np.max(mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30052715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sns.lineplot(np.log(S))\n",
    "#sns.lineplot(y=S)\n",
    "plt.plot(S)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a94323",
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = np.cumsum(S) / np.sum(S)\n",
    "r90 = np.min(np.where(cds > 0.90))\n",
    "r90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omega_approx(beta):\n",
    "    \"\"\"Return an approximate omega value for given beta. Equation (5) from Gavish 2014.\"\"\"\n",
    "    return 0.56 * beta**3 - 0.95 * beta**2 + 1.82 * beta + 1.43\n",
    "\n",
    "def svht(X, sigma=None, sv=None):\n",
    "    \"\"\"Return the optimal singular value hard threshold (SVHT) value.\n",
    "    `X` is any m-by-n matrix. `sigma` is the standard deviation of the \n",
    "    noise, if known. Optionally supply the vector of singular values `sv`\n",
    "    for the matrix (only necessary when `sigma` is unknown). If `sigma`\n",
    "    is unknown and `sv` is not supplied, then the method automatically\n",
    "    computes the singular values.\"\"\"\n",
    "\n",
    "    try:\n",
    "        m,n = sorted(X.shape) # ensures m <= n\n",
    "    except:\n",
    "        raise ValueError('invalid input matrix')\n",
    "    beta = m / n # ratio between 0 and 1\n",
    "    if sigma is None: # sigma unknown\n",
    "        if sv is None:\n",
    "            sv = svdvals(X)\n",
    "        sv = np.squeeze(sv)\n",
    "        if sv.ndim != 1:\n",
    "            raise ValueError('vector of singular values must be 1-dimensional')\n",
    "        return np.median(sv) * omega_approx(beta)\n",
    "    else: # sigma known\n",
    "        return lambda_star(beta) * np.sqrt(n) * sigma\n",
    "\n",
    "# find tau star hat when sigma is unknown\n",
    "# tau = svht(D, sv=sv)\n",
    "\n",
    "# # find tau star when sigma is known\n",
    "# tau = svht(D, sigma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = svht(mat, sv=S)\n",
    "tau\n",
    "rank = np.sum(S > tau)\n",
    "rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a455f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.dot(P, np.dot(np.diag(S), Q))\n",
    "\n",
    "print(\"Sum Recon: \", np.sum(B), \"Sum Orig: \", np.sum(mat), \"Diff: \", np.sum(B)-np.sum(mat))\n",
    "\n",
    "print(\"Orig Stats, Min: \", np.min(mat), \"Max: \", np.max(mat), \n",
    "      \"50p: \", np.percentile(mat, 50), \"90p:\", np.percentile(mat, 90), \n",
    "      \"99p: \", np.percentile(mat, 99), \"999p: \", np.percentile(mat, 99.9))\n",
    "\n",
    "print(\"Recon Stats, Min: \", np.min(B), \"Max: \", np.max(B), \n",
    "      \"50p: \", np.percentile(B, 50), \"90p:\", np.percentile(B, 90), \n",
    "      \"99p: \", np.percentile(B, 99), \"999p: \", np.percentile(mat, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P.shape, Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK  = 25\n",
    "B = np.dot(P[:,0:RANK], np.dot(np.diag(S[0:RANK]), Q[0:RANK,:]))\n",
    "print(\"Sum Recon: \", np.sum(B), \"Sum Orig: \", np.sum(mat), \"Diff: \", np.sum(B)-np.sum(mat))\n",
    "\n",
    "print(\"Orig Stats, Min: \", np.min(mat), \"Max: \", np.max(mat), \n",
    "      \"50p: \", np.percentile(mat, 50), \"90p:\", np.percentile(mat, 90), \n",
    "      \"99p: \", np.percentile(mat, 99), \"999p: \", np.percentile(mat, 99.9))\n",
    "\n",
    "print(\"Recon Stats, Min: \", np.min(B), \"Max: \", np.max(B), \n",
    "      \"50p: \", np.percentile(B, 50), \"90p:\", np.percentile(B, 90), \n",
    "      \"99p: \", np.percentile(B, 99), \"999p: \", np.percentile(mat, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_ests(estmat, rowidxs, qdata):\n",
    "    ests = []\n",
    "    \n",
    "    for qi, q in enumerate(qdata):\n",
    "        curests = {}\n",
    "        for node in q[\"subset_graph\"].nodes():\n",
    "            curests[node] = max(estmat[rowidxs[node], qi], 1.0)\n",
    "        ests.append(curests)\n",
    "    return ests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ef452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matests = matrix_to_ests(B, rowidxs, qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_alg(matests, EVAL_FNS, qdata, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost, AbsError\n",
    "\n",
    "# EVAL_FNS = []\n",
    "# #EVAL_FNS.append(SimplePlanCost())\n",
    "# EVAL_FNS.append(QError())\n",
    "# EVAL_FNS.append(PostgresPlanCost(cost_model=\"C\"))\n",
    "\n",
    "PG_PERRS = {}\n",
    "PG_PERRS[\"job2all\"] = 471442.0\n",
    "PG_PERRS[\"imdb-unique-plans1a\"] = 6208987\n",
    "PG_PERRS[\"imdb-unique-plansall-no7\"] = 17062409\n",
    "\n",
    "PG_QERRS = {}\n",
    "PG_QERRS[\"job2all\"] = 4974.446\n",
    "PG_QERRS[\"imdb-unique-plans1a\"] = 190.0\n",
    "PG_QERRS[\"imdb-unique-plansall-no7\"] = 89941\n",
    "\n",
    "TRUE_PERRS = {}\n",
    "TRUE_PERRS[\"job2all\"] = 159146.0\n",
    "TRUE_PERRS[\"imdb-unique-plans1a\"] = 1200973\n",
    "TRUE_PERRS[\"imdb-unique-plansall-no7\"] = 8022977\n",
    "\n",
    "PG_ABSERRS = {}\n",
    "PG_ABSERRS[\"job2all\"] = 6785997.991\n",
    "PG_ABSERRS[\"imdb-unique-plans1a\"] = 1282985\n",
    "PG_ABSERRS[\"imdb-unique-plansall-no7\"] = 41657676\n",
    "\n",
    "\n",
    "def get_rank_effects(mat, rowidxs, qdata):\n",
    "    \n",
    "    mean_qerrs = []\n",
    "    abs_errs = []\n",
    "    perrs = []\n",
    "    \n",
    "    ranks = []\n",
    "    \n",
    "    for rank in range(0, 100, 2):\n",
    "        \n",
    "        if rank == 0:\n",
    "            rank = 1\n",
    "        B = np.dot(P[:,0:rank], np.dot(np.diag(S[0:rank]), Q[0:rank,:]))\n",
    "        matests = matrix_to_ests(B, rowidxs, qdata)\n",
    "        \n",
    "        ranks.append(rank)\n",
    "        \n",
    "        qerr = QError()\n",
    "        qerrors = qerr.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0)\n",
    "     \n",
    "        mean_qerrs.append(np.mean(qerrors))\n",
    "        \n",
    "        abserr = AbsError()\n",
    "        abs_errs.append(np.mean(abserr.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0)))\n",
    "\n",
    "        ppc = PostgresPlanCost(cost_model=\"C\")\n",
    "        errors = ppc.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0,\n",
    "            user = \"ceb\",\n",
    "            db_name = \"imdb\",\n",
    "            db_host = \"localhost\",\n",
    "            password = \"password\",\n",
    "            port = 5432\n",
    "        )\n",
    "        perrs.append(np.mean(errors))\n",
    "        \n",
    "        print(ranks)\n",
    "        print(mean_qerrs)\n",
    "    \n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(30,6))\n",
    "    ax = axs[0]\n",
    "    sns.lineplot(x=ranks, y = mean_qerrs, ax = ax)\n",
    "    \n",
    "    pgqerr = PG_QERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=pgqerr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "    \n",
    "    ax.set_ylabel(\"QError\", fontsize=16)\n",
    "    ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax = axs[1]\n",
    "    sns.lineplot(x=ranks, y = abs_errs, ax = ax)\n",
    "    \n",
    "    ax.set_ylabel(\"Absolute Errors\", fontsize=16)\n",
    "    ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "    \n",
    "    pgaerr = PG_ABSERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=pgaerr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax = axs[2]\n",
    "    sns.lineplot(x=ranks, y = perrs, ax = ax)\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"Plan Costs\", fontsize=16)\n",
    "    ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "    \n",
    "    pgperr = PG_PERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=pgperr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "    \n",
    "    trueperr = TRUE_PERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=trueperr, xmin=1, xmax=ranks[-1], colors='g', linestyles='-', lw=4)\n",
    "    \n",
    "    FN_TMP = \"SVD-Recon-Errors-{}-{}.pdf\"\n",
    "    \n",
    "    FN = FN_TMP.format(QDIR, TEMPLATES)\n",
    "    \n",
    "    fig.suptitle(\"{}-{}\".format(QDIR, TEMPLATES), fontsize=20)\n",
    "    print(FN)\n",
    "    plt.savefig(FN, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    return ranks, mean_qerrs, abs_errs, perrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e51394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranks, qerrs, abserrs, perrs = get_rank_effects(mat, rowidxs, qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranks0, qerrs0, abserrs0, perrs0 = ranks, qerrs, abserrs, perrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048dda1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_qerrs = qerrs0 + qerrs\n",
    "# abs_errs = abserrs0 + abserrs\n",
    "# perrs = perrs0 + perrs\n",
    "# ranks = ranks0 + ranks\n",
    "\n",
    "# fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(30,6))\n",
    "# ax = axs[0]\n",
    "# sns.lineplot(x=ranks, y = mean_qerrs, ax = ax)\n",
    "\n",
    "# pgqerr = PG_QERRS[QDIR+TEMPLATES]\n",
    "# ax.hlines(y=pgqerr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "\n",
    "# ax.set_ylabel(\"QError\", fontsize=16)\n",
    "# ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "# ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "\n",
    "# ax.set_yscale(\"log\")\n",
    "\n",
    "# ax = axs[1]\n",
    "# sns.lineplot(x=ranks, y = abs_errs, ax = ax)\n",
    "\n",
    "# ax.set_ylabel(\"Absolute Errors\", fontsize=16)\n",
    "# ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "# ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "\n",
    "# pgaerr = PG_ABSERRS[QDIR+TEMPLATES]\n",
    "# ax.hlines(y=pgaerr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "\n",
    "# ax.set_yscale(\"log\")\n",
    "\n",
    "# ax = axs[2]\n",
    "# sns.lineplot(x=ranks, y = perrs, ax = ax)\n",
    "\n",
    "# ax.set_yscale(\"log\")\n",
    "# ax.set_ylabel(\"Plan Costs\", fontsize=16)\n",
    "# ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "# ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "\n",
    "# pgperr = PG_PERRS[QDIR+TEMPLATES]\n",
    "# ax.hlines(y=pgperr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "\n",
    "# trueperr = TRUE_PERRS[QDIR+TEMPLATES]\n",
    "# ax.hlines(y=trueperr, xmin=1, xmax=ranks[-1], colors='g', linestyles='-', lw=4)\n",
    "\n",
    "# FN_TMP = \"SVD-Recon-Errors-{}-{}.pdf\"\n",
    "\n",
    "# FN = FN_TMP.format(QDIR, TEMPLATES)\n",
    "\n",
    "# fig.suptitle(\"{}-{}\".format(QDIR, TEMPLATES), fontsize=20)\n",
    "# print(FN)\n",
    "# plt.savefig(FN, bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qerr(mat, newmat):\n",
    "    # find number of zeros in mat\n",
    "    tmp = mat[mat == 0]\n",
    " \n",
    "    mat = np.maximum(mat, 1)\n",
    "    newmat = np.maximum(newmat, 1)\n",
    "    #print(mat.shape)\n",
    "    qerrs = np.maximum (mat / newmat, newmat / mat)\n",
    "    print(\"QError --> Mean: {}, 50p: {}. 90p: {}, 99p: {}\".format(\n",
    "          np.mean(qerrs), np.percentile(qerrs,50), np.percentile(qerrs,90),\n",
    "          np.percentile(qerrs,99))\n",
    "         )\n",
    "    return np.mean(qerrs)\n",
    "\n",
    "def qerr_known(mat, newmat):\n",
    "    # find number of zeros in mat\n",
    "    tmp = copy.deepcopy(mat)\n",
    "    tmp[tmp != 0.0] = -1.0\n",
    "    #tmp[tmp >= 0.0] = 0.0\n",
    "    num_nonzeros = np.abs(np.sum(tmp))\n",
    " \n",
    "    mat = np.maximum(mat, 1)\n",
    "    newmat = np.maximum(newmat, 1)\n",
    "    #print(mat.shape)\n",
    "    qerrs = np.maximum (mat / newmat, newmat / mat)\n",
    "    \n",
    "    return np.sum(qerrs) / num_nonzeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f72776",
   "metadata": {},
   "source": [
    "# Creating a new matrix with some values as zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def create_incomplete_matrix(mat, mfrac):\n",
    "    newmat = np.zeros(mat.shape)\n",
    "\n",
    "    for col in range(mat.shape[1]):\n",
    "        #print(col)\n",
    "        curcol = copy.deepcopy(mat[:,col])\n",
    "        #print(curcol.shape)\n",
    "        indices = np.random.choice(np.arange(curcol.size), replace=False,\n",
    "                               size=int(curcol.size * mfrac))\n",
    "        curcol[indices] = 0.0\n",
    "        newmat[:,col] = curcol\n",
    "    \n",
    "    return newmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newmat = create_incomplete_matrix(mat, mfrac=0.1)\n",
    "# print(np.sum(mat), np.sum(newmat))\n",
    "# print(\"MSE: \", ((mat - newmat)**2).mean(axis=None))\n",
    "# print(\"QError: \", qerr(mat, newmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1702e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from fancyimpute import NuclearNormMinimization\n",
    "# from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "# #new_mat\n",
    "# newmat[newmat == 0] = np.nan\n",
    "\n",
    "# solver = KNN(\n",
    "#     min_value=1.0\n",
    "#     )\n",
    "\n",
    "# # X_incomplete has missing data which is represented with NaN values\n",
    "# mat_filled = solver.fit_transform(newmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359871a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qerr(mat, mat_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def zero_percentage(newmat):\n",
    "    tmp = copy.deepcopy(newmat)\n",
    "    tmp[np.isnan(tmp)] = -1.0\n",
    "    tmp[tmp == 0.0] = -1.0\n",
    "    tmp[tmp != -1.0] = 0\n",
    "    zeros = abs(np.sum(tmp))\n",
    "    total = tmp.shape[0]*tmp.shape[1]\n",
    "    return zeros / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mat, plan_mat, qdata, subplan_masks, rowidxs = load_plandata(qdata, rtdf)\n",
    "\n",
    "print(full_mat.shape, plan_mat.shape)\n",
    "print(zero_percentage(plan_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idxs = ~np.all(plan_mat == 0, axis=1)\n",
    "plan_mat = plan_mat[zero_idxs]\n",
    "full_mat = full_mat[zero_idxs]\n",
    "\n",
    "fmask = np.array(full_mat != 0, dtype=np.float32)\n",
    "pmask = np.array(plan_mat == 0, dtype=np.float32)\n",
    "\n",
    "print(full_mat.shape, plan_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce929c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerr(full_mat, plan_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b84a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_mat += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_mat = np.log(plan_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4117515",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(plan_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3eb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import NuclearNormMinimization\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "#new_mat\n",
    "tmp = copy.deepcopy(plan_mat)\n",
    "tmp[tmp == 0] = np.nan\n",
    "\n",
    "solver = KNN(\n",
    "    min_value=1.0\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc97ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_filled = np.exp(plan_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecbc273",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerr(full_mat, plan_filled)\n",
    "plan_filledm = plan_filled*fmask\n",
    "qerr(full_mat, plan_filledm)\n",
    "plan_filled2 = (plan_filled*fmask)*pmask\n",
    "full_mat2 = full_mat*pmask\n",
    "qerr(full_mat2, plan_filled2)\n",
    "print(\"QError Unknown: \", qerr_known(full_mat2, plan_filled2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f23e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = copy.deepcopy(plan_mat)\n",
    "# tmp[tmp == 0] = np.nan\n",
    "\n",
    "# solver = NuclearNormMinimization(\n",
    "#     min_value=1.0\n",
    "#     )\n",
    "\n",
    "# # X_incomplete has missing data which is represented with NaN values\n",
    "# plan_filled = solver.fit_transform(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1433f",
   "metadata": {},
   "source": [
    "# Training MSCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cardinality_estimation.mscn import MSCN as MSCN2\n",
    "from cardinality_estimation.mscn import MSCN as MSCN\n",
    "\n",
    "from cardinality_estimation.featurizer import Featurizer\n",
    "#from query_representation.query import load_qrep\n",
    "from cardinality_estimation.dataset import *\n",
    "\n",
    "max_epochs = 500\n",
    "lr=0.0001\n",
    "training_opt = \"none\"\n",
    "opt_lr = 0.1\n",
    "swa_start = 5\n",
    "mask_unseen_subplans = 0\n",
    "subplan_level_outputs=0\n",
    "normalize_flow_loss = 1\n",
    "heuristic_unseen_preds = 0\n",
    "heuristic_features = 1\n",
    "cost_model = \"C\"\n",
    "use_wandb = 0\n",
    "eval_fns = \"qerr,plancost\"\n",
    "load_padded_mscn_feats = 1\n",
    "mb_size = 1024\n",
    "weight_decay = 0.0\n",
    "load_query_together = 0\n",
    "result_dir = \"./results\"\n",
    "joinbitmap = True\n",
    "samplebitmap=False\n",
    "bitmapdir = os.path.join(\"../queries/allbitmaps_new/\", os.path.basename(TRAINDIR))\n",
    "\n",
    "print(TRAINDIR)\n",
    "print(\"BitmapDir: \", bitmapdir)\n",
    "\n",
    "onehot_dropout=2\n",
    "onehot_mask_truep=0.8\n",
    "onehot_reg=0\n",
    "onehot_reg_decay=0.1\n",
    "eval_epoch = 200\n",
    "optimizer_name=\"adamw\"\n",
    "clip_gradient=20.0\n",
    "loss_func_name = \"mse\"\n",
    "hidden_layer_size = 128\n",
    "num_hidden_layers = 2\n",
    "\n",
    "def init_featurizer(featurization_type, trainqs):\n",
    "    # Load database specific data, e.g., information about columns, tables etc.\n",
    "    dbdata_fn = os.path.join(TRAINDIR, \"dbdata.json\")\n",
    "    featurizer = Featurizer(None, None, None, None, None)\n",
    "    \n",
    "    with open(dbdata_fn, \"r\") as f:\n",
    "        dbdata = json.load(f)\n",
    "        \n",
    "    featurizer.update_using_saved_stats(dbdata)\n",
    "    \n",
    "    featurizer.setup(ynormalization=\"log\",\n",
    "        feat_separate_alias = 0,\n",
    "        onehot_dropout = onehot_dropout,\n",
    "        feat_mcvs = 0,\n",
    "        heuristic_features = heuristic_features,\n",
    "        featurization_type=featurization_type,\n",
    "        table_features=1,\n",
    "        flow_features = 0,\n",
    "        join_features= \"onehot\",\n",
    "        set_column_feature= \"onehot\",\n",
    "        max_discrete_featurizing_buckets=10,\n",
    "        max_like_featurizing_buckets=10,\n",
    "        embedding_fn = \"none\",\n",
    "        embedding_pooling = None,\n",
    "        implied_pred_features = 0,\n",
    "        feat_onlyseen_preds = 1,\n",
    "        bitmap_dir = bitmapdir,\n",
    "        join_bitmap = joinbitmap,\n",
    "        sample_bitmap = samplebitmap,\n",
    "                    )\n",
    "    featurizer.update_ystats(trainqs)\n",
    "    \n",
    "    featurizer.update_max_sets(trainqs)\n",
    "    featurizer.update_workload_stats(trainqs)\n",
    "    featurizer.init_feature_mapping()\n",
    "   \n",
    "\n",
    "    # if feat_onlyseen_preds:\n",
    "    # just do it always\n",
    "    featurizer.update_seen_preds(trainqs)\n",
    "    \n",
    "    return featurizer\n",
    "\n",
    "featurizer = init_featurizer(\"set\", qdata + tqdata)\n",
    "\n",
    "mscn = MSCN(max_epochs = max_epochs, lr=lr,\n",
    "                training_opt = training_opt,\n",
    "                inp_dropout = 0.0,\n",
    "                hl_dropout = 0.0,\n",
    "                comb_dropout = 0.0,\n",
    "                max_num_tables = -1,\n",
    "                opt_lr = opt_lr,\n",
    "                swa_start = swa_start,\n",
    "                mask_unseen_subplans = mask_unseen_subplans,\n",
    "                subplan_level_outputs=subplan_level_outputs,\n",
    "                normalize_flow_loss = normalize_flow_loss,\n",
    "                heuristic_unseen_preds = heuristic_unseen_preds,\n",
    "                cost_model = cost_model,\n",
    "                use_wandb = use_wandb,\n",
    "                eval_fns = eval_fns,\n",
    "                load_padded_mscn_feats = load_padded_mscn_feats,\n",
    "                mb_size = mb_size,\n",
    "                weight_decay = weight_decay,\n",
    "                load_query_together = load_query_together,\n",
    "                result_dir = result_dir,\n",
    "                onehot_dropout=onehot_dropout,\n",
    "                onehot_mask_truep=onehot_mask_truep,\n",
    "                onehot_reg=onehot_reg,\n",
    "                onehot_reg_decay=onehot_reg_decay,\n",
    "                # num_hidden_layers=num_hidden_layers,\n",
    "                save_mscn_feats = False,\n",
    "                eval_epoch = eval_epoch,\n",
    "                optimizer_name=optimizer_name,\n",
    "                clip_gradient=clip_gradient,\n",
    "                loss_func_name = loss_func_name,\n",
    "                hidden_layer_size = hidden_layer_size,\n",
    "                other_hid_units = hidden_layer_size,\n",
    "                num_hidden_layers = 2,\n",
    "                early_stopping = False,\n",
    "                random_bitmap_idx = False,\n",
    "                reg_loss = False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e66704",
   "metadata": {},
   "outputs": [],
   "source": [
    "mscn.train(qdata, valqs=[], testqs=[],\n",
    "    featurizer=featurizer, result_dir=\"results\",\n",
    "          subplan_mask=subplan_masks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost\n",
    "EVAL_FNS = []\n",
    "\n",
    "#EVAL_FNS.append(SimplePlanCost())\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(PostgresPlanCost(cost_model=\"C\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2481fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_alg(mscn, EVAL_FNS, qdata, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_alg(mscn, EVAL_FNS, tqdata, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981196b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = mscn.test(qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0418e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "estmat = np.ones((len(rowidxs), len(qdata)))\n",
    "\n",
    "for ei, est in enumerate(ests):\n",
    "    for k,v in est.items():\n",
    "        estmat[rowidxs[k], ei] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## matrices are only defined over subset of rows that are seen in data\n",
    "## fmask: only selects where true matrix has non-zeros\n",
    "## pmask: 1 only where plan_mat has zeros; i.e., unknown ones.\n",
    "\n",
    "estmat2 = estmat[zero_idxs]\n",
    "estmat2 = estmat2*fmask\n",
    "qerr(full_mat, estmat2)\n",
    "estmat2 = (estmat2*fmask)*pmask\n",
    "full_mat2 = full_mat*pmask\n",
    "\n",
    "qerr(full_mat2, estmat2)\n",
    "\n",
    "print(qerr_known(full_mat2, estmat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ec7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b53caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad825b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
