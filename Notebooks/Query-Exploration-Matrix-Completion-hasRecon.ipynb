{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "\n",
    "from query_representation.utils import *\n",
    "\n",
    "from torch.utils import data\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-separate",
   "metadata": {},
   "source": [
    "# Setup file paths / Download query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def make_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "def eval_alg(alg, eval_funcs, qreps, samples_type, result_dir=\"./results/\"):\n",
    "    '''\n",
    "    '''\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "    print(\"start eval alg\")\n",
    "    if not isinstance(alg, list):\n",
    "        ests = alg.test(qreps)\n",
    "        alg_name = alg.__str__()\n",
    "        exp_name = alg.get_exp_name()\n",
    "    else:\n",
    "        ests = alg\n",
    "        alg_name = \"Estimates\"\n",
    "        exp_name = \"test\"\n",
    "    \n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    \n",
    "    print(\"before eval funcs\")\n",
    "    for efunc in eval_funcs:\n",
    "        rdir = None\n",
    "        if result_dir is not None:\n",
    "            rdir = os.path.join(result_dir, exp_name)\n",
    "            make_dir(rdir)\n",
    "\n",
    "        errors = efunc.eval(qreps, ests, samples_type=samples_type,\n",
    "                result_dir=rdir,\n",
    "                num_processes = -1,\n",
    "                alg_name = alg_name,\n",
    "                use_wandb=0,\n",
    "                user = \"ceb\",\n",
    "                db_name = \"imdb\",\n",
    "                db_host = \"localhost\",\n",
    "                password = \"password\",\n",
    "                port = 5432\n",
    "                )\n",
    "\n",
    "        print(\"{}, {}, #samples: {}, {}: mean: {}, median: {}, 99p: {}\"\\\n",
    "                .format(samples_type, alg_name, len(errors),\n",
    "                    efunc.__str__(),\n",
    "                    np.round(np.mean(errors),3),\n",
    "                    np.round(np.median(errors),3),\n",
    "                    np.round(np.percentile(errors,99),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTDIR = os.path.join(os.path.join(\"..\", \"queries\"), \"imdb-unique-plans\")\n",
    "#RESULTDIR = os.path.join(\"..\", \"results\")\n",
    "#make_dir(RESULTDIR)\n",
    "\n",
    "MAXCARD = 150001000000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-collaboration",
   "metadata": {},
   "source": [
    "# Query loading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrep(fn):\n",
    "    assert \".pkl\" in fn\n",
    "    try:\n",
    "        with open(fn, \"rb\") as f:\n",
    "            query = pickle.load(f)\n",
    "    except:\n",
    "        print(fn + \" failed to load...\")\n",
    "        exit(-1)\n",
    "\n",
    "    query[\"subset_graph\"] = \\\n",
    "            nx.OrderedDiGraph(json_graph.adjacency_graph(query[\"subset_graph\"]))\n",
    "    query[\"join_graph\"] = json_graph.adjacency_graph(query[\"join_graph\"])\n",
    "    if \"subset_graph_paths\" in query:\n",
    "        query[\"subset_graph_paths\"] = \\\n",
    "                nx.OrderedDiGraph(json_graph.adjacency_graph(query[\"subset_graph_paths\"]))\n",
    "\n",
    "    return query\n",
    "\n",
    "def load_qdata(fns):\n",
    "    qreps = []\n",
    "    for qfn in fns:\n",
    "        qrep = load_qrep(qfn)\n",
    "        # TODO: can do checks like no queries with zero cardinalities etc.\n",
    "        qreps.append(qrep)\n",
    "        template_name = os.path.basename(os.path.dirname(qfn))\n",
    "        qrep[\"name\"] = os.path.basename(qfn)\n",
    "        qrep[\"template_name\"] = template_name\n",
    "    return qreps\n",
    "\n",
    "def get_query_fns(basedir, template_fraction=1.0, sel_templates=None):\n",
    "    fns = []\n",
    "    tmpnames = list(glob.glob(os.path.join(basedir, \"*\")))\n",
    "    print(tmpnames)\n",
    "    assert template_fraction <= 1.0\n",
    "    \n",
    "    if sel_templates == None:\n",
    "        sel_templates = \"all\"\n",
    "    \n",
    "    for qi,qdir in enumerate(tmpnames):\n",
    "        if os.path.isfile(qdir):\n",
    "            print(qdir)\n",
    "            continue\n",
    "        template_name = os.path.basename(qdir)\n",
    "        \n",
    "        if \"no7\" in sel_templates and template_name == \"7a\":\n",
    "            continue\n",
    "            \n",
    "        if \"all\" not in sel_templates and template_name not in sel_templates:\n",
    "            continue\n",
    "        \n",
    "        # let's first select all the qfns we are going to load\n",
    "        qfns = list(glob.glob(os.path.join(qdir, \"*.pkl\")))\n",
    "        qfns.sort()\n",
    "        num_samples = max(int(len(qfns)*template_fraction), 1)\n",
    "        random.seed(1234)\n",
    "        qfns = random.sample(qfns, num_samples)\n",
    "        fns += qfns\n",
    "    return fns\n",
    "\n",
    "def omega_approx(beta):\n",
    "    \"\"\"Return an approximate omega value for given beta. Equation (5) from Gavish 2014.\"\"\"\n",
    "    return 0.56 * beta**3 - 0.95 * beta**2 + 1.82 * beta + 1.43\n",
    "\n",
    "def svht(X, sigma=None, sv=None):\n",
    "    \"\"\"Return the optimal singular value hard threshold (SVHT) value.\n",
    "    `X` is any m-by-n matrix. `sigma` is the standard deviation of the \n",
    "    noise, if known. Optionally supply the vector of singular values `sv`\n",
    "    for the matrix (only necessary when `sigma` is unknown). If `sigma`\n",
    "    is unknown and `sv` is not supplied, then the method automatically\n",
    "    computes the singular values.\"\"\"\n",
    "\n",
    "    try:\n",
    "        m,n = sorted(X.shape) # ensures m <= n\n",
    "    except:\n",
    "        raise ValueError('invalid input matrix')\n",
    "    beta = m / n # ratio between 0 and 1\n",
    "    if sigma is None: # sigma unknown\n",
    "        if sv is None:\n",
    "            sv = svdvals(X)\n",
    "        sv = np.squeeze(sv)\n",
    "        if sv.ndim != 1:\n",
    "            raise ValueError('vector of singular values must be 1-dimensional')\n",
    "        return np.median(sv) * omega_approx(beta)\n",
    "    else: # sigma known\n",
    "        return lambda_star(beta) * np.sqrt(n) * sigma\n",
    "\n",
    "# find tau star hat when sigma is unknown\n",
    "# tau = svht(D, sv=sv)\n",
    "\n",
    "# # find tau star when sigma is known\n",
    "# tau = svht(D, sigma=0.5)\n",
    "\n",
    "import copy\n",
    "\n",
    "def zero_percentage(newmat):\n",
    "    tmp = copy.deepcopy(newmat)\n",
    "    tmp[np.isnan(tmp)] = -1.0\n",
    "    tmp[tmp == 0.0] = -1.0\n",
    "    tmp[tmp != -1.0] = 0\n",
    "    zeros = abs(np.sum(tmp))\n",
    "    total = tmp.shape[0]*tmp.shape[1]\n",
    "    return zeros / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-waterproof",
   "metadata": {},
   "source": [
    "# Evaluation helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-omaha",
   "metadata": {},
   "source": [
    "# Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDIR = \"imdb-unique-plans\"\n",
    "#QDIR = \"job2\"\n",
    "#TEMPLATES = \"all\"\n",
    "TEMPLATES = \"2b\"\n",
    "#TEMPLATES= \"no7_all\"\n",
    "TEST = \"2a\"\n",
    "\n",
    "#TEMPLATES = \"all-no7\"\n",
    "RTDIRS = [\"/flash1/pari/MyCEB/runtime_plans/pg\"]\n",
    "#RTDIRS = [\"/flash1/pari/MyCEB/runtime_plans/JOB/\"]\n",
    "\n",
    "TRAINDIR = os.path.join(os.path.join(\"/flash1/pari/MyCEB\", \"queries\"), QDIR)\n",
    "\n",
    "qfns = get_query_fns(TRAINDIR, template_fraction = 1.0, sel_templates=TEMPLATES)\n",
    "\n",
    "# TRAINDIR = os.path.join(os.path.join(\"/flash1/pari/MyCEB\", \"queries\"), \"job\")\n",
    "\n",
    "# qfns = get_query_fns(TRAINDIR, template_fraction = 1.0, sel_templates=None)\n",
    "\n",
    "print(len(qfns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be232a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdfs = []\n",
    "\n",
    "for RTDIR in RTDIRS:\n",
    "    rdirs = os.listdir(RTDIR)\n",
    "    for rd in rdirs:\n",
    "        rtfn = os.path.join(RTDIR, rd, \"Runtimes.csv\")\n",
    "        if os.path.exists(rtfn):\n",
    "            rtdfs.append(pd.read_csv(rtfn))\n",
    "            \n",
    "rtdf = pd.concat(rtdfs)\n",
    "print(\"Num RTs: \", len(rtdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33948ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy\n",
    "\n",
    "NUMTRAINQ = 10\n",
    "\n",
    "qdata = load_qdata(qfns)\n",
    "random.shuffle(qdata)\n",
    "\n",
    "testqdata = qdata[NUMTRAINQ:]\n",
    "\n",
    "print(len(testqdata))\n",
    "qdata = qdata[0:NUMTRAINQ]\n",
    "\n",
    "subplan_data = defaultdict(list)\n",
    "\n",
    "rowkeys = set()\n",
    "for qi, qrep in enumerate(qdata):\n",
    "    for node in qrep[\"subset_graph\"].nodes():\n",
    "#         if len(node) == 1:\n",
    "#             continue\n",
    "        rowkeys.add(node)\n",
    "    \n",
    "for qi, qrep in enumerate(testqdata):\n",
    "    for node in qrep[\"subset_graph\"].nodes():\n",
    "#         if len(node) == 1:\n",
    "#             continue\n",
    "        rowkeys.add(node)\n",
    "        \n",
    "rowkeys = list(rowkeys)\n",
    "rowkeys.sort()\n",
    "rowidxs = {rk:ri for ri,rk in enumerate(rowkeys)}\n",
    "\n",
    "mat = np.zeros((len(rowidxs), len(qdata)))\n",
    "pgmat = np.zeros((len(rowidxs), len(qdata)))\n",
    "\n",
    "for qi, qrep in enumerate(qdata):\n",
    "    for node in qrep[\"subset_graph\"].nodes():\n",
    "        if node not in rowidxs:\n",
    "            continue\n",
    "        truec = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"actual\"]\n",
    "        \n",
    "        if truec >= MAXCARD:\n",
    "            truec = 0.0\n",
    "            \n",
    "        mat[rowidxs[node], qi] = truec\n",
    "        pgmat[rowidxs[node], qi] = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"expected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75595ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_plandata(qdata, rtdf):\n",
    "    rowkeys = set()\n",
    "    rtdata = []\n",
    "    for qi, qrep in enumerate(qdata):\n",
    "        if qrep[\"name\"] not in rtdf[\"qname\"].values:\n",
    "            continue\n",
    "        rtdata.append(qrep)\n",
    "    \n",
    "    rowkeys = set()\n",
    "    for qi, qrep in enumerate(rtdata):\n",
    "        for node in qrep[\"subset_graph\"].nodes():\n",
    "            rowkeys.add(node)\n",
    "    rowkeys = list(rowkeys)\n",
    "    rowkeys.sort()\n",
    "    rowidxs = {rk:ri for ri,rk in enumerate(rowkeys)}\n",
    "    \n",
    "    mat = np.zeros((len(rowidxs), len(rtdata)))\n",
    "    pgmat = np.zeros((len(rowidxs), len(rtdata)))\n",
    "    tmat = np.zeros((len(rowidxs), len(rtdata)))\n",
    "    planmat = np.zeros((len(rowidxs), len(rtdata)))\n",
    "    subplan_masks = []\n",
    "    \n",
    "    for qi, qrep in enumerate(rtdata):\n",
    "        tmp = rtdf[rtdf[\"qname\"] == qrep[\"name\"]]\n",
    "        exp = tmp[\"exp_analyze\"].values[0]\n",
    "        try:\n",
    "            exp = eval(exp)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        G = explain_to_nx(exp)\n",
    "        seen_subplans = [ndata[\"aliases\"] for n,ndata in G.nodes(data=True)]\n",
    "        subplan_masks.append(seen_subplans)\n",
    "        \n",
    "        for node in qrep[\"subset_graph\"].nodes():\n",
    "            if node not in rowidxs:\n",
    "                continue\n",
    "            \n",
    "            truec = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"actual\"]\n",
    "            \n",
    "            if truec < MAXCARD:\n",
    "                mat[rowidxs[node], qi] = truec\n",
    "                pgmat[rowidxs[node], qi] = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"expected\"]\n",
    "                tmat[rowidxs[node], qi] = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"total\"]\n",
    "                \n",
    "            else:\n",
    "                mat[rowidxs[node], qi] = truec\n",
    "                pgmat[rowidxs[node], qi] = truec\n",
    "                tmat[rowidxs[node], qi] = truec\n",
    "            \n",
    "            if list(node) in seen_subplans:\n",
    "                planmat[rowidxs[node], qi] = truec\n",
    "                \n",
    "        numseen = 0\n",
    "        for node in qrep[\"subset_graph\"].nodes():\n",
    "            truec = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"actual\"]\n",
    "            if truec in planmat[:,qi]:\n",
    "                numseen += 1\n",
    "        \n",
    "#         print(\"Fraction in plan seen: \", numseen / len(qrep[\"subset_graph\"].nodes()), \", total: \", \n",
    "#               len(qrep[\"subset_graph\"].nodes()))\n",
    "            \n",
    "    return mat, pgmat, tmat, planmat, rtdata, subplan_masks, rowidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.maximum(mat, 1.0)\n",
    "logmat = np.log(mat)\n",
    "logP, logS, logV = np.linalg.svd(logmat, full_matrices=False)\n",
    "print(logmat.shape)\n",
    "print(logS.shape)\n",
    "print(logS.round(2))\n",
    "print(np.percentile(logS, 90), np.percentile(logP, 90), np.percentile(logV, 90))\n",
    "\n",
    "cds = np.cumsum(logS) / np.sum(logS)\n",
    "r90 = np.min(np.where(cds > 0.90))\n",
    "print(\"Threshold above 90p: \", r90)\n",
    "\n",
    "tau = svht(logmat, sv=logS)\n",
    "#tau\n",
    "rank = np.sum(logS > tau)\n",
    "print(\"rank according to noise calculation: \", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fb2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = np.maximum(mat, 1.0)\n",
    "# pgmat = np.maximum(pgmat, 1.0)\n",
    "# #mat = np.maximum(mat, 1.0)\n",
    "# logmat = mat / pgmat\n",
    "# logP, logS, logV = np.linalg.svd(logmat, full_matrices=False)\n",
    "# print(logmat.shape)\n",
    "# print(logS.shape)\n",
    "# print(logS.round(2))\n",
    "# print(np.percentile(logS, 90), np.percentile(logP, 90), np.percentile(logV, 90))\n",
    "\n",
    "# cds = np.cumsum(logS) / np.sum(logS)\n",
    "# r90 = np.min(np.where(cds > 0.90))\n",
    "# print(\"Threshold above 90p: \", r90)\n",
    "\n",
    "# tau = svht(logmat, sv=logS)\n",
    "# #tau\n",
    "# rank = np.sum(logS > tau)\n",
    "# print(\"rank according to noise calculation: \", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK  = 6\n",
    "lowlogmat = np.dot(logP[:,0:RANK], np.dot(np.diag(logS[0:RANK]), logV[0:RANK,:]))\n",
    "print(\"Sum Recon: \", np.sum(lowlogmat), \"Sum Orig: \", np.sum(logmat), \"Diff: \", np.sum(lowlogmat)-np.sum(logmat))\n",
    "\n",
    "print(\"Orig Stats, Min: \", np.min(logmat), \"Max: \", np.max(logmat), \n",
    "      \"50p: \", np.percentile(logmat, 50), \"90p:\", np.percentile(logmat, 90), \n",
    "      \"99p: \", np.percentile(logmat, 99), \"999p: \", np.percentile(logmat, 99.9))\n",
    "\n",
    "print(\"Recon Stats, Min: \", np.min(lowlogmat), \"Max: \", np.max(lowlogmat), \n",
    "      \"50p: \", np.percentile(lowlogmat, 50), \"90p:\", np.percentile(lowlogmat, 90), \n",
    "      \"99p: \", np.percentile(lowlogmat, 99), \"999p: \", np.percentile(mat, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5841b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import matrix_rank\n",
    "lowP = logP[:,0:RANK]\n",
    "lowS = logS[0:RANK]\n",
    "lowV = logV[0:RANK,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "idxs = deepcopy(rowidxs)\n",
    "idxs = sorted(idxs, key=lambda x: len(x))\n",
    "\n",
    "# TODO: remove linearly dependent guys from here\n",
    "newrows = []\n",
    "newrowidxs = []\n",
    "prevrank = 0\n",
    "for i, idx in enumerate(idxs):\n",
    "    if prevrank >= RANK:\n",
    "        break\n",
    "    newrows.append(lowP[rowidxs[idx]])\n",
    "    \n",
    "    \n",
    "    newrank = matrix_rank(newrows)\n",
    "    print(\"{}, New rank: {}\".format(idx, newrank))\n",
    "    \n",
    "    if newrank == prevrank:\n",
    "        newrows = newrows[0:-1]\n",
    "    else:\n",
    "        newrowidxs.append(rowidxs[idx])\n",
    "        \n",
    "    prevrank = newrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d966b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newrows = np.array(newrows)\n",
    "newrows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ae641a",
   "metadata": {},
   "source": [
    "# Single Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c7f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowP[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb3f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninv = np.linalg.inv(newrows)\n",
    "coeff = lowP[2] @ ninv\n",
    "print(coeff)\n",
    "print(logS)\n",
    "lowP2recon = (coeff @ newrows)\n",
    "np.allclose(lowP2recon, lowP[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcd9d2",
   "metadata": {},
   "source": [
    "# Coefficients for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninv = np.linalg.inv(newrows)\n",
    "coeffs = {}\n",
    "\n",
    "#for i in range(len(lowP)):\n",
    "    \n",
    "for row,ridx in rowidxs.items():\n",
    "    if ridx in newrowidxs:\n",
    "        continue\n",
    "    coeff = lowP[ridx] @ ninv\n",
    "    coeffs[row] = coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efecd234",
   "metadata": {},
   "source": [
    "# Reconstruct training data matrix based on coeffs\n",
    "## Shape: NumRows x RANK; RANK Singular Vals; RANK x NumCols; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ae72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efd984bd",
   "metadata": {},
   "source": [
    "# Reconstruct based on part of column;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bbf8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lowP.shape, lowS.shape, np.diag(lowS).shape)\n",
    "print((lowP @ np.diag(lowS)).shape)\n",
    "recon = lowP @ np.diag(lowS) @ lowV\n",
    "print(recon.shape)\n",
    "print(np.allclose(recon, lowlogmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a026f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lowlogmat[:,0])\n",
    "print(logmat[:,0])\n",
    "print(\"******\")\n",
    "print(lowlogmat[newrowidxs, 0])\n",
    "print(logmat[newrowidxs, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec28c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost, AbsError\n",
    "\n",
    "def eval_comp_mat(mat, qdata, rowidxs):\n",
    "    ## calculate losses based on Xrecon\n",
    "    matests = matrix_to_ests(mat, rowidxs, qdata)\n",
    "\n",
    "    qerr = QError()\n",
    "    qerrors = qerr.eval(qdata, matests, samples_type=\"train\",\n",
    "                result_dir=None,\n",
    "                num_processes = -1,\n",
    "                alg_name = \"SVD\",\n",
    "                use_wandb=0)\n",
    "\n",
    "\n",
    "    ppc = PostgresPlanCost(cost_model=\"C\")\n",
    "    errors = ppc.eval(qdata, matests, samples_type=\"train\",\n",
    "        result_dir=None,\n",
    "        num_processes = -1,\n",
    "        alg_name = \"SVD\",\n",
    "        use_wandb=0,\n",
    "        user = \"ceb\",\n",
    "        db_name = \"imdb\",\n",
    "        db_host = \"localhost\",\n",
    "        password = \"password\",\n",
    "        port = 5432\n",
    "    )\n",
    "\n",
    "    print(\"Calculating losses: \")\n",
    "    print(\"QErrors: \", np.mean(qerrors))\n",
    "    print(\"PPC / 1e6: \", np.mean(errors) / 1e6)\n",
    "    \n",
    "def matrix_to_ests(estmat, rowidxs, qdata):\n",
    "    ests = []\n",
    "    \n",
    "    for qi, q in enumerate(qdata):\n",
    "        curests = {}\n",
    "        for node in q[\"subset_graph\"].nodes():\n",
    "            curests[node] = max(estmat[rowidxs[node], qi], 1.0)\n",
    "        ests.append(curests)\n",
    "    return ests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e4e969",
   "metadata": {},
   "source": [
    "# Solving for most of Q, sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73587586",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = newrows @ np.diag(lowS)\n",
    "print(left.shape)\n",
    "print(lowV.shape)\n",
    "## we want to recover lowV\n",
    "leftinv = np.linalg.inv(left)\n",
    "\n",
    "Xrankrows = lowlogmat[newrowidxs, :]\n",
    "\n",
    "Vrecon = (leftinv @ Xrankrows)\n",
    "\n",
    "print(Vrecon.shape)\n",
    "print(lowV.shape)\n",
    "\n",
    "## select the rank r part of Q based on the rows we selected.\n",
    "print(np.allclose(Vrecon, lowV))\n",
    "\n",
    "print(np.sum(Vrecon))\n",
    "#print(np.sum(Vrows))\n",
    "print(np.sum(lowV))\n",
    "\n",
    "Xrecon = np.dot(logP[:, 0:RANK], np.dot(np.diag(logS[0:RANK]), Vrecon))\n",
    "\n",
    "print(\"Recovers the low rank approx: \", np.allclose(Xrecon, lowlogmat))\n",
    "print(\"Compared to the original data: \", np.allclose(Xrecon, logmat))\n",
    "\n",
    "Xrecon = np.exp(Xrecon)\n",
    "## calculate losses based on Xrecon\n",
    "matests = matrix_to_ests(Xrecon, rowidxs, qdata)\n",
    "\n",
    "qerr = QError()\n",
    "qerrors = qerr.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0)\n",
    "\n",
    "\n",
    "ppc = PostgresPlanCost(cost_model=\"C\")\n",
    "errors = ppc.eval(qdata, matests, samples_type=\"train\",\n",
    "    result_dir=None,\n",
    "    num_processes = -1,\n",
    "    alg_name = \"SVD\",\n",
    "    use_wandb=0,\n",
    "    user = \"ceb\",\n",
    "    db_name = \"imdb\",\n",
    "    db_host = \"localhost\",\n",
    "    password = \"password\",\n",
    "    port = 5432\n",
    ")\n",
    "\n",
    "print(\"Calculating losses: \")\n",
    "print(\"QErrors: \", np.mean(qerrors))\n",
    "print(\"PPC: \", np.mean(errors) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae59c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Vrecon[:,0])\n",
    "print(logV[0:RANK,0])\n",
    "\n",
    "print(\"******\")\n",
    "\n",
    "print(Xrecon[:, 0])\n",
    "print(np.exp(logmat[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb368e77",
   "metadata": {},
   "source": [
    "# Using true estimates instead of low rank mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = newrows @ np.diag(lowS)\n",
    "print(left.shape)\n",
    "print(lowV.shape)\n",
    "## we want to recover lowV\n",
    "leftinv = np.linalg.inv(left)\n",
    "\n",
    "Xrankrows = logmat[newrowidxs, :]\n",
    "\n",
    "Vrecon = (leftinv @ Xrankrows)\n",
    "\n",
    "print(Vrecon.shape)\n",
    "print(lowV.shape)\n",
    "\n",
    "## select the rank r part of Q based on the rows we selected.\n",
    "print(np.allclose(Vrecon, lowV))\n",
    "\n",
    "print(\"sum Vrecon: \", np.sum(Vrecon))\n",
    "#print(np.sum(Vrows))\n",
    "print(\"sum lowRankV: \", np.sum(lowV))\n",
    "\n",
    "#Xrecon = np.dot(logP[:, 0:RANK], np.dot(np.diag(logS[0:RANK]), Vrecon))\n",
    "Xrecon = (logP[:, 0:RANK] @ np.diag(logS[0:RANK])) @ Vrecon\n",
    "\n",
    "print(\"Recovers the low rank approx: \", np.allclose(Xrecon, lowlogmat))\n",
    "print(\"Compared to the original data: \", np.allclose(Xrecon, logmat))\n",
    "\n",
    "Xrecon = np.exp(Xrecon)\n",
    "Xrecon = np.maximum(Xrecon, 1.0)\n",
    "## calculate losses based on Xrecon\n",
    "matests = matrix_to_ests(Xrecon, rowidxs, qdata)\n",
    "\n",
    "qerr = QError()\n",
    "qerrors = qerr.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0)\n",
    "\n",
    "\n",
    "ppc = PostgresPlanCost(cost_model=\"C\")\n",
    "errors = ppc.eval(qdata, matests, samples_type=\"train\",\n",
    "    result_dir=None,\n",
    "    num_processes = -1,\n",
    "    alg_name = \"SVD\",\n",
    "    use_wandb=0,\n",
    "    user = \"ceb\",\n",
    "    db_name = \"imdb\",\n",
    "    db_host = \"localhost\",\n",
    "    password = \"password\",\n",
    "    port = 5432\n",
    ")\n",
    "\n",
    "print(\"Calculating losses: \")\n",
    "print(\"QErrors: \", np.mean(qerrors))\n",
    "print(\"PPC: \", np.mean(errors) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5899a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logmat[newrowidxs, :])\n",
    "print(\"low rank log matrix: \")\n",
    "print(lowlogmat[newrowidxs, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ccb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Vrecon[:,0])\n",
    "print(logV[0:RANK,0])\n",
    "\n",
    "print(\"*******\")\n",
    "print(Xrecon[:, 0])\n",
    "print(np.exp(logmat[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104fa8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3c255d3",
   "metadata": {},
   "source": [
    "# Matrix Completion, for new queries at QO time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4509637",
   "metadata": {},
   "outputs": [],
   "source": [
    "testmat = np.zeros((len(rowidxs), len(testqdata)))\n",
    "testpgmat = np.zeros((len(rowidxs), len(testqdata)))\n",
    "\n",
    "for qi, qrep in enumerate(testqdata):\n",
    "    for node in qrep[\"subset_graph\"].nodes():\n",
    "        if node not in rowidxs:\n",
    "            continue\n",
    "        truec = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"actual\"]\n",
    "        \n",
    "#         if truec >= MAXCARD:\n",
    "#             truec = 0.0\n",
    "        \n",
    "        ## or create some other list of accepted row idxs\n",
    "#         if rowidxs[node] in newrowidxs:  \n",
    "#             testmat[rowidxs[node], qi] = truec\n",
    "#             testpgmat[rowidxs[node], qi] = qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"expected\"]\n",
    "        \n",
    "        if len(node) <= 1:\n",
    "            testmat[rowidxs[node], qi] = max(truec, 1.0)\n",
    "            testpgmat[rowidxs[node], qi] = max(qrep[\"subset_graph\"].nodes()[node][\"cardinality\"][\"expected\"], 1.0)\n",
    "        else:\n",
    "            testmat[rowidxs[node], qi] = np.nan\n",
    "            testpgmat[rowidxs[node], qi] = np.nan\n",
    "        \n",
    "# add np.nan's in testmat\n",
    "testmat += 1\n",
    "testpgmat += 1\n",
    "testlogmat = np.log(testmat)\n",
    "testlogpgmat = np.log(testpgmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ad5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMTRAINQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0fe59",
   "metadata": {},
   "source": [
    "# With partial true data in the new queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ecca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qdata), len(testqdata), combmat.shape, testlogmat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edc62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import NuclearNormMinimization\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "combmat = np.hstack([logmat, testlogmat])\n",
    "tmp = copy.deepcopy(combmat)\n",
    "\n",
    "solver = KNN(\n",
    "    min_value=1.0\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)\n",
    "plan_filled = np.exp(plan_filled)\n",
    "plan_filled = plan_filled[:,NUMTRAINQ:]\n",
    "print(plan_filled.shape)\n",
    "eval_comp_mat(plan_filled, testqdata, rowidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import NuclearNormMinimization\n",
    "from fancyimpute import KNN, MatrixFactorization\n",
    "combmat = np.hstack([logmat, testlogmat])\n",
    "tmp = copy.deepcopy(combmat)\n",
    "\n",
    "solver = MatrixFactorization(\n",
    "    min_value=1.0, rank = 10\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)\n",
    "plan_filled = np.exp(plan_filled)\n",
    "plan_filled = plan_filled[:,NUMTRAINQ:]\n",
    "eval_comp_mat(plan_filled, testqdata, rowidxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ce3ab",
   "metadata": {},
   "source": [
    "# With partial postgres data in the new queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcaede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import NuclearNormMinimization\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "combmat = np.hstack([logmat, testlogpgmat])\n",
    "tmp = copy.deepcopy(combmat)\n",
    "\n",
    "solver = KNN(\n",
    "    min_value=1.0\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)\n",
    "plan_filled = np.exp(plan_filled)\n",
    "plan_filled = plan_filled[:,NUMTRAINQ:]\n",
    "eval_comp_mat(plan_filled, testqdata, rowidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import NuclearNormMinimization\n",
    "from fancyimpute import KNN, MatrixFactorization\n",
    "combmat = np.hstack([logmat, testlogpgmat])\n",
    "tmp = copy.deepcopy(combmat)\n",
    "\n",
    "solver = MatrixFactorization(\n",
    "    min_value=1.0, rank = 10\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)\n",
    "plan_filled = np.exp(plan_filled)\n",
    "plan_filled = plan_filled[:,NUMTRAINQ:]\n",
    "eval_comp_mat(plan_filled, testqdata, rowidxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4549e1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_comp_mat(mat, qdata, rowidxs):\n",
    "#     ## calculate losses based on Xrecon\n",
    "#     matests = matrix_to_ests(mat, rowidxs, qdata)\n",
    "\n",
    "#     qerr = QError()\n",
    "#     qerrors = qerr.eval(qdata, matests, samples_type=\"train\",\n",
    "#                 result_dir=None,\n",
    "#                 num_processes = -1,\n",
    "#                 alg_name = \"SVD\",\n",
    "#                 use_wandb=0)\n",
    "\n",
    "\n",
    "#     ppc = PostgresPlanCost(cost_model=\"C\")\n",
    "#     errors = ppc.eval(qdata, matests, samples_type=\"train\",\n",
    "#         result_dir=None,\n",
    "#         num_processes = -1,\n",
    "#         alg_name = \"SVD\",\n",
    "#         use_wandb=0,\n",
    "#         user = \"ceb\",\n",
    "#         db_name = \"imdb\",\n",
    "#         db_host = \"localhost\",\n",
    "#         password = \"password\",\n",
    "#         port = 5432\n",
    "#     )\n",
    "\n",
    "#     print(\"Calculating losses: \")\n",
    "#     print(\"QErrors: \", np.mean(qerrors))\n",
    "#     print(\"PPC: \", np.mean(errors) / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f62e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d774d421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sns.lineplot(np.log(S))\n",
    "#sns.lineplot(y=S)\n",
    "plt.plot(logS)\n",
    "#plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1bd165",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, S, Q = np.linalg.svd(mat, full_matrices=False)\n",
    "print(mat.shape)\n",
    "print(S.shape)\n",
    "print(np.percentile(S, 90), np.percentile(P, 90), np.percentile(Q, 90))\n",
    "print(np.percentile(S, 90) - np.max(mat))\n",
    "print(np.max(mat))\n",
    "cds = np.cumsum(S) / np.sum(S)\n",
    "r90 = np.min(np.where(cds > 0.90))\n",
    "print(\"90p point: \", r90)\n",
    "\n",
    "tau = svht(mat, sv=S)\n",
    "tau\n",
    "rank_noise = np.sum(S > tau)\n",
    "print(\"rank according to noise threshold: \", rank_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30052715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sns.lineplot(np.log(S))\n",
    "#sns.lineplot(y=S)\n",
    "\n",
    "#markers_on = [12, 17, 18, 19]\n",
    "#plt.plot(xs, ys, '-gD', markevery=markers_on, label='line with select markers')\n",
    "\n",
    "plt.plot(S, \"-gD\", markevery=[r90, rank_noise])\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "FN = \"SingularVals-{}-{}.pdf\".format(QDIR, TEMPLATES)\n",
    "\n",
    "plt.savefig(FN, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968b8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mat.shape)\n",
    "zero_percentage(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.dot(P, np.dot(np.diag(S), Q))\n",
    "\n",
    "print(\"Sum Recon: \", np.sum(B), \"Sum Orig: \", np.sum(mat), \"Diff: \", np.sum(B)-np.sum(mat))\n",
    "\n",
    "print(\"Orig Stats, Min: \", np.min(mat), \"Max: \", np.max(mat), \n",
    "      \"50p: \", np.percentile(mat, 50), \"90p:\", np.percentile(mat, 90), \n",
    "      \"99p: \", np.percentile(mat, 99), \"999p: \", np.percentile(mat, 99.9))\n",
    "\n",
    "print(\"Recon Stats, Min: \", np.min(B), \"Max: \", np.max(B), \n",
    "      \"50p: \", np.percentile(B, 50), \"90p:\", np.percentile(B, 90), \n",
    "      \"99p: \", np.percentile(B, 99), \"999p: \", np.percentile(mat, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P.shape, Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK  = 25\n",
    "B = np.dot(P[:,0:RANK], np.dot(np.diag(S[0:RANK]), Q[0:RANK,:]))\n",
    "print(\"Sum Recon: \", np.sum(B), \"Sum Orig: \", np.sum(mat), \"Diff: \", np.sum(B)-np.sum(mat))\n",
    "\n",
    "print(\"Orig Stats, Min: \", np.min(mat), \"Max: \", np.max(mat), \n",
    "      \"50p: \", np.percentile(mat, 50), \"90p:\", np.percentile(mat, 90), \n",
    "      \"99p: \", np.percentile(mat, 99), \"999p: \", np.percentile(mat, 99.9))\n",
    "\n",
    "print(\"Recon Stats, Min: \", np.min(B), \"Max: \", np.max(B), \n",
    "      \"50p: \", np.percentile(B, 50), \"90p:\", np.percentile(B, 90), \n",
    "      \"99p: \", np.percentile(B, 99), \"999p: \", np.percentile(mat, 99.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a68ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ef452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matests = matrix_to_ests(B, rowidxs, qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_alg(matests, EVAL_FNS, qdata, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost, AbsError\n",
    "\n",
    "# EVAL_FNS = []\n",
    "# #EVAL_FNS.append(SimplePlanCost())\n",
    "# EVAL_FNS.append(QError())\n",
    "# EVAL_FNS.append(PostgresPlanCost(cost_model=\"C\"))\n",
    "\n",
    "PG_PERRS = {}\n",
    "PG_PERRS[\"job2all\"] = 471442.0\n",
    "PG_PERRS[\"imdb-unique-plans1a\"] = 6208987\n",
    "PG_PERRS[\"imdb-unique-plansall-no7\"] = 17062409\n",
    "\n",
    "PG_QERRS = {}\n",
    "PG_QERRS[\"job2all\"] = 4974.446\n",
    "PG_QERRS[\"imdb-unique-plans1a\"] = 190.0\n",
    "PG_QERRS[\"imdb-unique-plansall-no7\"] = 89941\n",
    "\n",
    "TRUE_PERRS = {}\n",
    "TRUE_PERRS[\"job2all\"] = 159146.0\n",
    "TRUE_PERRS[\"imdb-unique-plans1a\"] = 1200973\n",
    "TRUE_PERRS[\"imdb-unique-plansall-no7\"] = 8022977\n",
    "\n",
    "PG_ABSERRS = {}\n",
    "PG_ABSERRS[\"job2all\"] = 6785997.991\n",
    "PG_ABSERRS[\"imdb-unique-plans1a\"] = 1282985\n",
    "PG_ABSERRS[\"imdb-unique-plansall-no7\"] = 41657676\n",
    "\n",
    "\n",
    "def get_rank_effects(mat, rowidxs, qdata, log=False):\n",
    "    \n",
    "    mean_qerrs = []\n",
    "    abs_errs = []\n",
    "    perrs = []\n",
    "    \n",
    "    ranks = []\n",
    "    \n",
    "    if log:\n",
    "        if np.min(mat) == 0:\n",
    "            mat += 1\n",
    "        mat = np.log(mat)\n",
    "        P, S, Q = np.linalg.svd(mat, full_matrices=False) \n",
    "    \n",
    "    for rank in range(0, 100, 10):\n",
    "        \n",
    "        if rank == 0:\n",
    "            rank = 1\n",
    "        B = np.dot(P[:,0:rank], np.dot(np.diag(S[0:rank]), Q[0:rank,:]))\n",
    "        if log:\n",
    "            B = np.exp(B)\n",
    "            \n",
    "        matests = matrix_to_ests(B, rowidxs, qdata)\n",
    "        \n",
    "        ranks.append(rank)\n",
    "        \n",
    "        qerr = QError()\n",
    "        qerrors = qerr.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0)\n",
    "     \n",
    "        mean_qerrs.append(np.mean(qerrors))\n",
    "        \n",
    "        abserr = AbsError()\n",
    "        abs_errs.append(np.mean(abserr.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0)))\n",
    "\n",
    "        ppc = PostgresPlanCost(cost_model=\"C\")\n",
    "        errors = ppc.eval(qdata, matests, samples_type=\"train\",\n",
    "            result_dir=None,\n",
    "            num_processes = -1,\n",
    "            alg_name = \"SVD\",\n",
    "            use_wandb=0,\n",
    "            user = \"ceb\",\n",
    "            db_name = \"imdb\",\n",
    "            db_host = \"localhost\",\n",
    "            password = \"password\",\n",
    "            port = 5432\n",
    "        )\n",
    "        perrs.append(np.mean(errors))\n",
    "        \n",
    "        print(ranks)\n",
    "        print(mean_qerrs)\n",
    "    \n",
    "    fig,axs = plt.subplots(nrows=1,ncols=3,figsize=(30,6))\n",
    "    ax = axs[0]\n",
    "    sns.lineplot(x=ranks, y = mean_qerrs, ax = ax)\n",
    "    \n",
    "    pgqerr = PG_QERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=pgqerr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "    \n",
    "    ax.set_ylabel(\"QError\", fontsize=16)\n",
    "    ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax = axs[1]\n",
    "    sns.lineplot(x=ranks, y = abs_errs, ax = ax)\n",
    "    \n",
    "    ax.set_ylabel(\"Absolute Errors\", fontsize=16)\n",
    "    ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "    \n",
    "    pgaerr = PG_ABSERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=pgaerr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    \n",
    "    ax = axs[2]\n",
    "    sns.lineplot(x=ranks, y = perrs, ax = ax)\n",
    "    \n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_ylabel(\"Plan Costs\", fontsize=16)\n",
    "    ax.set_xlabel(\"Rank\", fontsize=16)\n",
    "    ax.tick_params(axis='both', which='both', labelsize=16)\n",
    "    \n",
    "    pgperr = PG_PERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=pgperr, xmin=1, xmax=ranks[-1], colors='r', linestyles='-', lw=4)\n",
    "    \n",
    "    trueperr = TRUE_PERRS[QDIR+TEMPLATES]\n",
    "    ax.hlines(y=trueperr, xmin=1, xmax=ranks[-1], colors='g', linestyles='-', lw=4)\n",
    "    \n",
    "    FN_TMP = \"SVD-Recon-Errors-{}-{}-{}.pdf\"\n",
    "    \n",
    "    FN = FN_TMP.format(QDIR, TEMPLATES, log)\n",
    "    \n",
    "    fig.suptitle(\"{}-{}\".format(QDIR, TEMPLATES), fontsize=20)\n",
    "    print(FN)\n",
    "    plt.savefig(FN, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    return ranks, mean_qerrs, abs_errs, perrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f3f7e",
   "metadata": {},
   "source": [
    "# Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fce4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranks, qerrs, abserrs, perrs = get_rank_effects(mat, rowidxs, qdata, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranks, qerrs, abserrs, perrs = get_rank_effects(mat, rowidxs, qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qerr(mat, newmat):\n",
    "    # find number of zeros in mat\n",
    "    tmp = mat[mat == 0]\n",
    " \n",
    "    mat = np.maximum(mat, 1)\n",
    "    newmat = np.maximum(newmat, 1)\n",
    "    #print(mat.shape)\n",
    "    qerrs = np.maximum (mat / newmat, newmat / mat)\n",
    "    print(\"QError --> Mean: {}, 50p: {}. 90p: {}, 99p: {}\".format(\n",
    "          np.mean(qerrs), np.percentile(qerrs,50), np.percentile(qerrs,90),\n",
    "          np.percentile(qerrs,99))\n",
    "         )\n",
    "    return np.mean(qerrs)\n",
    "\n",
    "def qerr_known(mat, newmat):\n",
    "    # find number of zeros in mat\n",
    "    tmp = copy.deepcopy(mat)\n",
    "    tmp[tmp != 0.0] = -1.0\n",
    "    #tmp[tmp >= 0.0] = 0.0\n",
    "    num_nonzeros = np.abs(np.sum(tmp))\n",
    " \n",
    "    mat = np.maximum(mat, 1)\n",
    "    newmat = np.maximum(newmat, 1)\n",
    "    #print(mat.shape)\n",
    "    qerrs = np.maximum (mat / newmat, newmat / mat)\n",
    "    \n",
    "    return np.sum(qerrs) / num_nonzeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f72776",
   "metadata": {},
   "source": [
    "# Creating a new matrix with some values as zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def create_incomplete_matrix(mat, mfrac):\n",
    "    newmat = np.zeros(mat.shape)\n",
    "\n",
    "    for col in range(mat.shape[1]):\n",
    "        #print(col)\n",
    "        curcol = copy.deepcopy(mat[:,col])\n",
    "        #print(curcol.shape)\n",
    "        indices = np.random.choice(np.arange(curcol.size), replace=False,\n",
    "                               size=int(curcol.size * mfrac))\n",
    "        curcol[indices] = 0.0\n",
    "        newmat[:,col] = curcol\n",
    "    \n",
    "    return newmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newmat = create_incomplete_matrix(mat, mfrac=0.1)\n",
    "# print(np.sum(mat), np.sum(newmat))\n",
    "# print(\"MSE: \", ((mat - newmat)**2).mean(axis=None))\n",
    "# print(\"QError: \", qerr(mat, newmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1702e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from fancyimpute import NuclearNormMinimization\n",
    "# from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "# #new_mat\n",
    "# newmat[newmat == 0] = np.nan\n",
    "\n",
    "# solver = KNN(\n",
    "#     min_value=1.0\n",
    "#     )\n",
    "\n",
    "# # X_incomplete has missing data which is represented with NaN values\n",
    "# mat_filled = solver.fit_transform(newmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359871a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qerr(mat, mat_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251675c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mat, pgmat, tmat, plan_mat, qdata, subplan_masks, rowidxs = load_plandata(qdata, rtdf)\n",
    "\n",
    "#tfull_mat,tpgmat,ttmat, tplan_mat, tqdata, tsubplan_masks, trowidxs = load_plandata(qdata, rtdf)\n",
    "\n",
    "print(full_mat.shape, plan_mat.shape)\n",
    "print(zero_percentage(plan_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a364561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idxs = ~np.all(plan_mat == 0, axis=1)\n",
    "plan_matz = plan_mat[zero_idxs]\n",
    "full_matz = full_mat[zero_idxs]\n",
    "\n",
    "fmask = np.array(full_mat != 0, dtype=np.float32)\n",
    "pmask = np.array(plan_mat == 0, dtype=np.float32)\n",
    "\n",
    "fmaskz = np.array(full_matz != 0, dtype=np.float32)\n",
    "pmaskz = np.array(plan_matz == 0, dtype=np.float32)\n",
    "\n",
    "print(full_mat.shape, plan_mat.shape)\n",
    "print(full_matz.shape, plan_matz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce929c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerr(full_mat, plan_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea85198b",
   "metadata": {},
   "source": [
    "# Predicting cardinality / pg_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d74095",
   "metadata": {},
   "outputs": [],
   "source": [
    "pgmat = np.maximum(pgmat, 1.0)\n",
    "plan_mat2 = plan_mat / pgmat\n",
    "print(np.max(plan_mat2), np.min(plan_mat2), np.mean(plan_mat2))\n",
    "\n",
    "from fancyimpute import IterativeSVD, SoftImpute, BiScaler, MatrixFactorization, IterativeImputer\n",
    "\n",
    "tmp = copy.deepcopy(plan_mat2)\n",
    "tmp[tmp == 0] = np.nan\n",
    "\n",
    "tmp = np.log(tmp)\n",
    "\n",
    "solver = MatrixFactorization(\n",
    "    min_value=0.0,\n",
    "    rank = 80\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)\n",
    "print(np.max(plan_filled), np.min(plan_filled), np.mean(plan_filled))\n",
    "\n",
    "plan_filled = np.exp(plan_filled)\n",
    "plan_filled = plan_filled * pgmat\n",
    "\n",
    "qerr(full_mat, plan_filled)\n",
    "plan_filledm = plan_filled*fmask\n",
    "qerr(full_mat, plan_filledm)\n",
    "plan_filled2 = (plan_filled*fmask)*pmask\n",
    "full_mat2 = full_mat*pmask\n",
    "qerr(full_mat2, plan_filled2)\n",
    "print(\"QError Unknown: \", qerr_known(full_mat2, plan_filled2))\n",
    "\n",
    "from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost\n",
    "EVAL_FNS = []\n",
    "matests = matrix_to_ests(plan_filled, rowidxs, qdata)\n",
    "\n",
    "#EVAL_FNS.append(SimplePlanCost())\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(PostgresPlanCost(cost_model=\"C\"))\n",
    "eval_alg(matests, EVAL_FNS, qdata, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d5d40",
   "metadata": {},
   "source": [
    "# Predicting cardinality / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412166cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fancyimpute import IterativeSVD, SoftImpute, BiScaler, MatrixFactorization, IterativeImputer\n",
    "\n",
    "# pgmat = np.maximum(pgmat, 1.0)\n",
    "# plan_mat2 = plan_mat / tmat\n",
    "# print(np.max(plan_mat2), np.min(plan_mat2), np.mean(plan_mat2))\n",
    "\n",
    "# tmp = copy.deepcopy(plan_mat2)\n",
    "\n",
    "# tmp[tmp == 0] = np.nan\n",
    "\n",
    "# solver = MatrixFactorization(\n",
    "#     min_value=0.03,\n",
    "#     rank = 40,\n",
    "#     )\n",
    "\n",
    "# # X_incomplete has missing data which is represented with NaN values\n",
    "# plan_filled = solver.fit_transform(tmp)\n",
    "# print(np.max(plan_filled), np.min(plan_filled), np.mean(plan_filled))\n",
    "\n",
    "# plan_filled = plan_filled * tmat\n",
    "\n",
    "# qerr(full_mat, plan_filled)\n",
    "# plan_filledm = plan_filled*fmask\n",
    "# qerr(full_mat, plan_filledm)\n",
    "# plan_filled2 = (plan_filled*fmask)*pmask\n",
    "# full_mat2 = full_mat*pmask\n",
    "# qerr(full_mat2, plan_filled2)\n",
    "# print(\"QError Unknown: \", qerr_known(full_mat2, plan_filled2))\n",
    "\n",
    "# from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost\n",
    "# EVAL_FNS = []\n",
    "# matests = matrix_to_ests(plan_filled, rowidxs, qdata)\n",
    "\n",
    "# #EVAL_FNS.append(SimplePlanCost())\n",
    "# EVAL_FNS.append(QError())\n",
    "# EVAL_FNS.append(PostgresPlanCost(cost_model=\"C\"))\n",
    "# eval_alg(matests, EVAL_FNS, qdata, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970cbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c169aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_mat += 1\n",
    "plan_mat = np.log(plan_mat)\n",
    "print(\"Min: \", np.min(plan_mat))\n",
    "\n",
    "plan_matz += 1\n",
    "plan_matz = np.log(plan_matz)\n",
    "print(\"Min: \", np.min(plan_matz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3eb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import NuclearNormMinimization\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "#new_mat\n",
    "tmp = copy.deepcopy(plan_matz)\n",
    "tmp[tmp == 0] = np.nan\n",
    "\n",
    "solver = KNN(\n",
    "    min_value=1.0\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)\n",
    "plan_filled = np.exp(plan_filled)\n",
    "\n",
    "qerr(full_matz, plan_filled)\n",
    "plan_filledm = plan_filled*fmaskz\n",
    "qerr(full_matz, plan_filledm)\n",
    "plan_filled2 = (plan_filled*fmaskz)*pmaskz\n",
    "full_mat2 = full_matz*pmaskz\n",
    "qerr(full_mat2, plan_filled2)\n",
    "print(\"QError Unknown: \", qerr_known(full_mat2, plan_filled2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f458d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from fancyimpute import NuclearNormMinimization\n",
    "from fancyimpute import IterativeSVD, SoftImpute, BiScaler, MatrixFactorization, IterativeImputer\n",
    "#new_mat\n",
    "tmp = copy.deepcopy(plan_mat)\n",
    "tmp[tmp == 0] = np.nan\n",
    "\n",
    "solver = MatrixFactorization(\n",
    "    min_value=1.0,\n",
    "    #rank = 10\n",
    "    )\n",
    "\n",
    "# X_incomplete has missing data which is represented with NaN values\n",
    "plan_filled = solver.fit_transform(tmp)\n",
    "plan_filled = np.exp(plan_filled)\n",
    "\n",
    "qerr(full_mat, plan_filled)\n",
    "plan_filledm = plan_filled*fmask\n",
    "qerr(full_mat, plan_filledm)\n",
    "plan_filled2 = (plan_filled*fmask)*pmask\n",
    "full_mat2 = full_mat*pmask\n",
    "qerr(full_mat2, plan_filled2)\n",
    "print(\"QError Unknown: \", qerr_known(full_mat2, plan_filled2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f796e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e09618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost\n",
    "EVAL_FNS = []\n",
    "\n",
    "matests = matrix_to_ests(plan_filled, rowidxs, qdata)\n",
    "#EVAL_FNS.append(SimplePlanCost())\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(PostgresPlanCost(cost_model=\"C\"))\n",
    "eval_alg(matests, EVAL_FNS, qdata, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(plan_filled2), np.mean(plan_filled2), np.max(plan_filled2))\n",
    "print(np.min(full_mat2), np.mean(full_mat2), np.max(full_mat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from fancyimpute import NuclearNormMinimization\n",
    "# from fancyimpute import IterativeSVD, SoftImpute, BiScaler, MatrixFactorization, IterativeImputer\n",
    "# #new_mat\n",
    "# tmp = copy.deepcopy(plan_mat)\n",
    "# tmp[tmp == 0] = np.nan\n",
    "\n",
    "# solver = IterativeImputer(\n",
    "#     min_value=1.0\n",
    "#     )\n",
    "\n",
    "# # X_incomplete has missing data which is represented with NaN values\n",
    "# plan_filled = solver.fit_transform(tmp)\n",
    "# plan_filled = np.exp(plan_filled)\n",
    "\n",
    "# qerr(full_mat, plan_filled)\n",
    "# plan_filledm = plan_filled*fmask\n",
    "# qerr(full_mat, plan_filledm)\n",
    "# plan_filled2 = (plan_filled*fmask)*pmask\n",
    "# full_mat2 = full_mat*pmask\n",
    "# qerr(full_mat2, plan_filled2)\n",
    "# print(\"QError Unknown: \", qerr_known(full_mat2, plan_filled2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a857b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from fancyimpute import NuclearNormMinimization\n",
    "# from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "# #new_mat\n",
    "# tmp = copy.deepcopy(plan_mat)\n",
    "# tmp[tmp == 0] = np.nan\n",
    "\n",
    "# solver = NuclearNormMinimization(\n",
    "#     min_value=1.0\n",
    "#     )\n",
    "\n",
    "# # X_incomplete has missing data which is represented with NaN values\n",
    "# plan_filled = solver.fit_transform(tmp)\n",
    "# plan_filled = np.exp(plan_filled)\n",
    "\n",
    "# qerr(full_mat, plan_filled)\n",
    "# plan_filledm = plan_filled*fmask\n",
    "# qerr(full_mat, plan_filledm)\n",
    "# plan_filled2 = (plan_filled*fmask)*pmask\n",
    "# full_mat2 = full_mat*pmask\n",
    "# qerr(full_mat2, plan_filled2)\n",
    "# print(\"QError Unknown: \", qerr_known(full_mat2, plan_filled2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1433f",
   "metadata": {},
   "source": [
    "# Training MSCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee7a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cardinality_estimation.mscn import MSCN as MSCN2\n",
    "from cardinality_estimation.mscn import MSCN as MSCN\n",
    "\n",
    "from cardinality_estimation.featurizer import Featurizer\n",
    "#from query_representation.query import load_qrep\n",
    "from cardinality_estimation.dataset import *\n",
    "\n",
    "max_epochs = 100\n",
    "lr=0.001\n",
    "training_opt = \"none\"\n",
    "opt_lr = 0.1\n",
    "swa_start = 5\n",
    "mask_unseen_subplans = 0\n",
    "subplan_level_outputs=0\n",
    "normalize_flow_loss = 1\n",
    "heuristic_unseen_preds = 0\n",
    "heuristic_features = 1\n",
    "cost_model = \"C\"\n",
    "use_wandb = 0\n",
    "eval_fns = \"qerr,plancost\"\n",
    "load_padded_mscn_feats = 1\n",
    "mb_size = 1024\n",
    "weight_decay = 0.0\n",
    "load_query_together = 0\n",
    "result_dir = \"./results\"\n",
    "joinbitmap = False\n",
    "samplebitmap=False\n",
    "bitmapdir = os.path.join(\"../queries/allbitmaps_new/\", os.path.basename(TRAINDIR))\n",
    "print(TRAINDIR)\n",
    "print(\"BitmapDir: \", bitmapdir)\n",
    "\n",
    "onehot_dropout=0\n",
    "onehot_mask_truep=0.8\n",
    "onehot_reg=0\n",
    "onehot_reg_decay=0.1\n",
    "eval_epoch = 200\n",
    "optimizer_name=\"adamw\"\n",
    "clip_gradient=20.0\n",
    "loss_func_name = \"mse\"\n",
    "hidden_layer_size = 128\n",
    "num_hidden_layers = 2\n",
    "\n",
    "def init_featurizer(featurization_type, trainqs):\n",
    "    # Load database specific data, e.g., information about columns, tables etc.\n",
    "    dbdata_fn = os.path.join(TRAINDIR, \"dbdata.json\")\n",
    "    featurizer = Featurizer(None, None, None, None, None)\n",
    "    \n",
    "    with open(dbdata_fn, \"r\") as f:\n",
    "        dbdata = json.load(f)\n",
    "        \n",
    "    featurizer.update_using_saved_stats(dbdata)\n",
    "    \n",
    "    featurizer.setup(ynormalization=\"log\",\n",
    "        feat_separate_alias = 0,\n",
    "        onehot_dropout = onehot_dropout,\n",
    "        feat_mcvs = 0,\n",
    "        heuristic_features = heuristic_features,\n",
    "        featurization_type=featurization_type,\n",
    "        table_features=1,\n",
    "        flow_features = 0,\n",
    "        join_features= \"onehot\",\n",
    "        set_column_feature= \"onehot\",\n",
    "        max_discrete_featurizing_buckets=10,\n",
    "        max_like_featurizing_buckets=10,\n",
    "        embedding_fn = \"none\",\n",
    "        embedding_pooling = None,\n",
    "        implied_pred_features = 0,\n",
    "        feat_onlyseen_preds = 1,\n",
    "        bitmap_dir = bitmapdir,\n",
    "        join_bitmap = joinbitmap,\n",
    "        sample_bitmap = samplebitmap,\n",
    "                    )\n",
    "    featurizer.update_ystats(trainqs)\n",
    "    \n",
    "    featurizer.update_max_sets(trainqs)\n",
    "    featurizer.update_workload_stats(trainqs)\n",
    "    featurizer.init_feature_mapping()\n",
    "   \n",
    "\n",
    "    # if feat_onlyseen_preds:\n",
    "    # just do it always\n",
    "    featurizer.update_seen_preds(trainqs)\n",
    "    \n",
    "    return featurizer\n",
    "\n",
    "featurizer = init_featurizer(\"set\", qdata)\n",
    "\n",
    "mscn = MSCN(max_epochs = max_epochs, lr=lr,\n",
    "                training_opt = training_opt,\n",
    "                inp_dropout = 0.0,\n",
    "                hl_dropout = 0.0,\n",
    "                comb_dropout = 0.0,\n",
    "                max_num_tables = -1,\n",
    "                opt_lr = opt_lr,\n",
    "                swa_start = swa_start,\n",
    "                mask_unseen_subplans = mask_unseen_subplans,\n",
    "                subplan_level_outputs=subplan_level_outputs,\n",
    "                normalize_flow_loss = normalize_flow_loss,\n",
    "                heuristic_unseen_preds = heuristic_unseen_preds,\n",
    "                cost_model = cost_model,\n",
    "                use_wandb = use_wandb,\n",
    "                eval_fns = eval_fns,\n",
    "                load_padded_mscn_feats = load_padded_mscn_feats,\n",
    "                mb_size = mb_size,\n",
    "                weight_decay = weight_decay,\n",
    "                load_query_together = load_query_together,\n",
    "                result_dir = result_dir,\n",
    "                onehot_dropout=onehot_dropout,\n",
    "                onehot_mask_truep=onehot_mask_truep,\n",
    "                onehot_reg=onehot_reg,\n",
    "                onehot_reg_decay=onehot_reg_decay,\n",
    "                # num_hidden_layers=num_hidden_layers,\n",
    "                save_mscn_feats = False,\n",
    "                eval_epoch = eval_epoch,\n",
    "                optimizer_name=optimizer_name,\n",
    "                clip_gradient=clip_gradient,\n",
    "                loss_func_name = loss_func_name,\n",
    "                hidden_layer_size = hidden_layer_size,\n",
    "                other_hid_units = hidden_layer_size,\n",
    "                num_hidden_layers = 2,\n",
    "                early_stopping = False,\n",
    "                random_bitmap_idx = False,\n",
    "                reg_loss = False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e7867",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e66704",
   "metadata": {},
   "outputs": [],
   "source": [
    "mscn.train(qdata, valqs=[], testqs=[],\n",
    "    featurizer=featurizer, result_dir=\"results\",\n",
    "          subplan_mask=subplan_masks\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16ca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost, PostgresPlanCost\n",
    "EVAL_FNS = []\n",
    "\n",
    "#EVAL_FNS.append(SimplePlanCost())\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(PostgresPlanCost(cost_model=\"C\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2481fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_alg(mscn, EVAL_FNS, qdata, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981196b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = mscn.test(qdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0418e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "estmat = np.ones((len(rowidxs), len(qdata)))\n",
    "\n",
    "for ei, est in enumerate(ests):\n",
    "    for k,v in est.items():\n",
    "        estmat[rowidxs[k], ei] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## matrices are only defined over subset of rows that are seen in data\n",
    "## fmask: only selects where true matrix has non-zeros\n",
    "## pmask: 1 only where plan_mat has zeros; i.e., unknown ones.\n",
    "\n",
    "#estmat2 = estmat[zero_idxs]\n",
    "estmat2 = estmat\n",
    "estmat2 = estmat2*fmask\n",
    "qerr(full_mat, estmat2)\n",
    "estmat2 = (estmat2*fmask)*pmask\n",
    "full_mat2 = full_mat*pmask\n",
    "\n",
    "qerr(full_mat2, estmat2)\n",
    "\n",
    "print(qerr_known(full_mat2, estmat2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572ec7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b53caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad825b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
