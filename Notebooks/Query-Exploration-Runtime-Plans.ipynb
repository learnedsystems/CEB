{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "#from cardinality_estimation.featurizer import Featurizer\n",
    "\n",
    "from query_representation.query import *\n",
    "from query_representation.utils import *\n",
    "from cardinality_estimation.dataset import *\n",
    "\n",
    "from torch.utils import data\n",
    "import pickle\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TESTDIR = os.path.join(os.path.join(\"..\", \"queries\"), \"imdb-unique-plans\")\n",
    "#RESULTDIR = os.path.join(\"..\", \"results\")\n",
    "#make_dir(RESULTDIR)\n",
    "\n",
    "TRAINDIR = os.path.join(os.path.join(\"/flash1/pari/MyCEB\", \"queries\"), \"imdb-unique-plans\")\n",
    "\n",
    "RTDIRS = [\"/flash1/pari/MyCEB/runtime_plans/pg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdfs = []\n",
    "\n",
    "for RTDIR in RTDIRS:    \n",
    "    rdirs = os.listdir(RTDIR)\n",
    "    for rd in rdirs:\n",
    "        rtfn = os.path.join(RTDIR, rd, \"Runtimes.csv\")\n",
    "        if os.path.exists(rtfn):\n",
    "            rtdfs.append(pd.read_csv(rtfn))\n",
    "rtdf = pd.concat(rtdfs)\n",
    "print(\"Num RTs: \", len(rtdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-collaboration",
   "metadata": {},
   "source": [
    "# Query loading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qrep(fn):\n",
    "    assert \".pkl\" in fn\n",
    "    try:\n",
    "        with open(fn, \"rb\") as f:\n",
    "            query = pickle.load(f)\n",
    "    except:\n",
    "        print(fn + \" failed to load...\")\n",
    "        exit(-1)\n",
    "\n",
    "    query[\"subset_graph\"] = \\\n",
    "            nx.OrderedDiGraph(json_graph.adjacency_graph(query[\"subset_graph\"]))\n",
    "    query[\"join_graph\"] = json_graph.adjacency_graph(query[\"join_graph\"])\n",
    "    if \"subset_graph_paths\" in query:\n",
    "        query[\"subset_graph_paths\"] = \\\n",
    "                nx.OrderedDiGraph(json_graph.adjacency_graph(query[\"subset_graph_paths\"]))\n",
    "\n",
    "    return query\n",
    "\n",
    "\n",
    "def load_qdata(fns):\n",
    "    qreps = []\n",
    "    for qfn in fns:\n",
    "        qrep = load_qrep(qfn)\n",
    "        #qrep = load_sql_qrep(qfn)\n",
    "        # TODO: can do checks like no queries with zero cardinalities etc.\n",
    "        qreps.append(qrep)\n",
    "        template_name = os.path.basename(os.path.dirname(qfn))\n",
    "        qrep[\"name\"] = os.path.basename(qfn)\n",
    "        qrep[\"template_name\"] = template_name\n",
    "    return qreps\n",
    "\n",
    "def get_query_fns(basedir, template_fraction=1.0, sel_templates=None):\n",
    "    fns = []\n",
    "    tmpnames = list(glob.glob(os.path.join(basedir, \"*\")))\n",
    "    print(tmpnames)\n",
    "    assert template_fraction <= 1.0\n",
    "    \n",
    "    for qi,qdir in enumerate(tmpnames):\n",
    "        if os.path.isfile(qdir):\n",
    "            print(qdir)\n",
    "            continue\n",
    "        template_name = os.path.basename(qdir)\n",
    "        if sel_templates is not None and template_name not in sel_templates:\n",
    "            continue\n",
    "        \n",
    "        # let's first select all the qfns we are going to load\n",
    "        qfns = list(glob.glob(os.path.join(qdir, \"*.pkl\")))\n",
    "        qfns.sort()\n",
    "        num_samples = max(int(len(qfns)*template_fraction), 1)\n",
    "        random.seed(1234)\n",
    "        qfns = random.sample(qfns, num_samples)\n",
    "        fns += qfns\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-waterproof",
   "metadata": {},
   "source": [
    "# Evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def omega_approx(beta):\n",
    "    \"\"\"Return an approximate omega value for given beta. Equation (5) from Gavish 2014.\"\"\"\n",
    "    return 0.56 * beta**3 - 0.95 * beta**2 + 1.82 * beta + 1.43\n",
    "\n",
    "def svht(X, sigma=None, sv=None):\n",
    "    \"\"\"Return the optimal singular value hard threshold (SVHT) value.\n",
    "    `X` is any m-by-n matrix. `sigma` is the standard deviation of the \n",
    "    noise, if known. Optionally supply the vector of singular values `sv`\n",
    "    for the matrix (only necessary when `sigma` is unknown). If `sigma`\n",
    "    is unknown and `sv` is not supplied, then the method automatically\n",
    "    computes the singular values.\"\"\"\n",
    "\n",
    "    try:\n",
    "        m,n = sorted(X.shape) # ensures m <= n\n",
    "    except:\n",
    "        raise ValueError('invalid input matrix')\n",
    "    beta = m / n # ratio between 0 and 1\n",
    "    if sigma is None: # sigma unknown\n",
    "        if sv is None:\n",
    "            sv = svdvals(X)\n",
    "        sv = np.squeeze(sv)\n",
    "        if sv.ndim != 1:\n",
    "            raise ValueError('vector of singular values must be 1-dimensional')\n",
    "        return np.median(sv) * omega_approx(beta)\n",
    "    else: # sigma known\n",
    "        return lambda_star(beta) * np.sqrt(n) * sigma\n",
    "\n",
    "# find tau star hat when sigma is unknown\n",
    "# tau = svht(D, sv=sv)\n",
    "\n",
    "# # find tau star when sigma is known\n",
    "# tau = svht(D, sigma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-omaha",
   "metadata": {},
   "source": [
    "# Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set template_fraction <= 1.0 to test quickly w/ smaller datasets\n",
    "# train_qfns = get_query_fns(TRAINDIR, template_fraction = 0.001)\n",
    "# val_qfns = get_query_fns(VALDIR, template_fraction = 1.0)\n",
    "# test_qfns = get_query_fns(TESTDIR, template_fraction = 1.0)\n",
    "\n",
    "#qfns = get_query_fns(TRAINDIR, template_fraction = 1.0, sel_templates=None)\n",
    "\n",
    "qfns = get_query_fns(TRAINDIR, template_fraction = 1.0, sel_templates=None)\n",
    "print(len(qfns))\n",
    "qdata = load_qdata(qfns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e9dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = qdata[0]\n",
    "sg = q[\"subset_graph\"]\n",
    "sg.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy\n",
    "\n",
    "subplan_data = defaultdict(list)\n",
    "\n",
    "rowkeys = set()\n",
    "\n",
    "cur_exps = []\n",
    "\n",
    "qid = 0\n",
    "newqs = {}\n",
    "\n",
    "for qi, qrep in enumerate(qdata):\n",
    "    for node in qrep[\"subset_graph\"].nodes():\n",
    "        rowkeys.add(node)\n",
    "        \n",
    "    tmp = rtdf[rtdf[\"qname\"] == qrep[\"name\"]]\n",
    "    \n",
    "    if len(tmp) != 0:\n",
    "        for explain in tmp[\"exp_analyze\"].values:\n",
    "            newqs[len(cur_exps)] = qid\n",
    "            cur_exps.append(explain)\n",
    "        qid += 1\n",
    "    \n",
    "rowkeys = list(rowkeys)\n",
    "rowkeys.sort()\n",
    "rowidxs = {rk:ri for ri,rk in enumerate(rowkeys)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtmat = np.zeros((len(rowidxs), qid))\n",
    "print(rtmat.shape)\n",
    "\n",
    "# ['tables', 'aliases', 'Plan Rows', 'Actual Rows', 'total_time', 'cur_time', \n",
    "#  'Node Type', 'Total Cost', 'cur_cost', 'node_label', 'scan_type']\n",
    "\n",
    "for ei, exp in enumerate(cur_exps):\n",
    "    try:\n",
    "        exp = eval(exp)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    G = explain_to_nx(exp)\n",
    "    cur_qid = newqs[ei]\n",
    "    for node,ndata in G.nodes(data=True):\n",
    "        cal = ndata[\"aliases\"]\n",
    "        cal.sort()\n",
    "        rt = ndata[\"cur_time\"]\n",
    "        #print(ndata.keys())\n",
    "        rtmat[rowidxs[tuple(cal)], cur_qid] = rt\n",
    "        \n",
    "rtmat = rtmat[~np.all(rtmat == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = np.count_nonzero(rtmat)\n",
    "tot = rtmat.shape[0]*rtmat.shape[1]\n",
    "print(\"Non Zero Fraction: \", nz / tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, S, Q = np.linalg.svd(rtmat, full_matrices=False)\n",
    "print(S.shape)\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(S)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = np.cumsum(S) / np.sum(S)\n",
    "r90 = np.min(np.where(cds > 0.90))\n",
    "print(\"90% explained by: \", r90)\n",
    "\n",
    "tau = svht(rtmat, sv=S)\n",
    "rank = np.sum(S > tau)\n",
    "print(\"Noise cut-off: \", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "logrtmat = copy.deepcopy(rtmat)\n",
    "logrtmat += 1\n",
    "\n",
    "logrtmat = np.log(logrtmat)\n",
    "P, S, Q = np.linalg.svd(logrtmat, full_matrices=False)\n",
    "print(S.shape)\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ec140",
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = np.cumsum(S) / np.sum(S)\n",
    "r90 = np.min(np.where(cds > 0.90))\n",
    "print(\"90% explained by: \", r90)\n",
    "\n",
    "tau = svht(rtmat, sv=S)\n",
    "rank = np.sum(S > tau)\n",
    "print(\"Noise cut-off: \", rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4993fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
