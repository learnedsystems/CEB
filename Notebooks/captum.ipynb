{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from cardinality_estimation.featurizer import Featurizer\n",
    "from query_representation.query import load_qrep\n",
    "from cardinality_estimation.dataset import *\n",
    "from torch.utils import data\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-separate",
   "metadata": {},
   "source": [
    "# Setup file paths / Download query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def make_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINDIR = os.path.join(os.path.join(\"..\", \"queries\"), \"imdb-unique-plans\")\n",
    "\n",
    "TRAINDIR = os.path.join(os.path.join(\"..\", \"queries\"), \"joblight_train\")\n",
    "TESTDIR = os.path.join(os.path.join(\"..\", \"queries\"), \"imdb-unique-plans\")\n",
    "RESULTDIR = os.path.join(\"..\", \"results\")\n",
    "make_dir(RESULTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-collaboration",
   "metadata": {},
   "source": [
    "# Query loading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_qdata(fns):\n",
    "    qreps = []\n",
    "    for qfn in fns:\n",
    "        qrep = load_qrep(qfn)\n",
    "        # TODO: can do checks like no queries with zero cardinalities etc.\n",
    "        qreps.append(qrep)\n",
    "        template_name = os.path.basename(os.path.dirname(qfn))\n",
    "        qrep[\"name\"] = os.path.basename(qfn)\n",
    "        qrep[\"template_name\"] = template_name\n",
    "        qrep[\"workload\"] = \"imdb\"\n",
    "    return qreps\n",
    "\n",
    "def get_query_fns(basedir, template_fraction=1.0, sel_templates=None):\n",
    "    fns = []\n",
    "    tmpnames = list(glob.glob(os.path.join(basedir, \"*\")))\n",
    "    assert template_fraction <= 1.0\n",
    "    \n",
    "    for qi,qdir in enumerate(tmpnames):\n",
    "        if os.path.isfile(qdir):\n",
    "            continue\n",
    "        template_name = os.path.basename(qdir)\n",
    "        if sel_templates is not None and template_name not in sel_templates:\n",
    "            continue\n",
    "        \n",
    "        # let's first select all the qfns we are going to load\n",
    "        qfns = list(glob.glob(os.path.join(qdir, \"*.pkl\")))\n",
    "        qfns.sort()\n",
    "        num_samples = max(int(len(qfns)*template_fraction), 1)\n",
    "        random.seed(1234)\n",
    "        qfns = random.sample(qfns, num_samples)\n",
    "        fns += qfns\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-waterproof",
   "metadata": {},
   "source": [
    "# Evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(alg, qreps):\n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    ests = alg.test(qreps)\n",
    "    return ests\n",
    "\n",
    "def eval_alg(alg, eval_funcs, qreps, samples_type, result_dir=\"./results/\"):\n",
    "    '''\n",
    "    '''\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "    alg_name = alg.__str__()\n",
    "    exp_name = alg.get_exp_name()\n",
    "    \n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    ests = alg.test(qreps)\n",
    "\n",
    "    for efunc in eval_funcs:\n",
    "        rdir = None\n",
    "        if result_dir is not None:\n",
    "            rdir = os.path.join(result_dir, exp_name)\n",
    "            make_dir(rdir)\n",
    "\n",
    "        errors = efunc.eval(qreps, ests, samples_type=samples_type,\n",
    "                result_dir=rdir,\n",
    "                num_processes = -1,\n",
    "                alg_name = alg_name,\n",
    "                use_wandb=0)\n",
    "\n",
    "        print(\"{}, {}, #samples: {}, {}: mean: {}, median: {}, 99p: {}\"\\\n",
    "                .format(samples_type, alg, len(errors),\n",
    "                    efunc.__str__(),\n",
    "                    np.round(np.mean(errors),3),\n",
    "                    np.round(np.median(errors),3),\n",
    "                    np.round(np.percentile(errors,99),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-omaha",
   "metadata": {},
   "source": [
    "# Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set template_fraction <= 1.0 to test quickly w/ smaller datasets\n",
    "# train_qfns = get_query_fns(TRAINDIR, template_fraction = 0.001)\n",
    "# val_qfns = get_query_fns(VALDIR, template_fraction = 1.0)\n",
    "# test_qfns = get_query_fns(TESTDIR, template_fraction = 1.0)\n",
    "\n",
    "train_qfns = get_query_fns(TRAINDIR, template_fraction = 0.5)\n",
    "val_qfns = []\n",
    "test_qfns = get_query_fns(TESTDIR, template_fraction = 1.0, sel_templates=[\"2a\"])\n",
    "\n",
    "print(\"Selected {} training queries, {} validation queries, {} test queries\".\\\n",
    "      format(len(train_qfns), len(val_qfns), len(test_qfns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost\n",
    "EVAL_FNS = []\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(SimplePlanCost())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_featurizer(featurization_type):\n",
    "    # Load database specific data, e.g., information about columns, tables etc.\n",
    "    dbdata_fn = os.path.join(TRAINDIR, \"dbdata.json\")\n",
    "    featurizer = Featurizer(\"user\", \"pwd\", \"imdb\", None, None)\n",
    "    \n",
    "    \n",
    "    with open(dbdata_fn, \"r\") as f:\n",
    "        dbdata = json.load(f)\n",
    "        \n",
    "    featurizer.update_using_saved_stats(dbdata)\n",
    "    \n",
    "    featurizer.setup(ynormalization=\"log\",\n",
    "        feat_separate_alias = 0,\n",
    "        onehot_dropout = onehot_dropout,\n",
    "        feat_mcvs = 0,\n",
    "        heuristic_features = 1,\n",
    "        featurization_type=featurization_type,\n",
    "        table_features=1,\n",
    "        flow_features = 0,\n",
    "        join_features= \"onehot\",\n",
    "        set_column_feature= \"onehot\",\n",
    "        max_discrete_featurizing_buckets=10,\n",
    "        max_like_featurizing_buckets=10,\n",
    "        embedding_fn = \"none\",\n",
    "        embedding_pooling = None,\n",
    "        implied_pred_features = 0,\n",
    "        feat_onlyseen_preds = 1)\n",
    "    featurizer.update_ystats(trainqs)\n",
    "    \n",
    "    featurizer.update_max_sets(trainqs)\n",
    "    featurizer.update_workload_stats(trainqs)\n",
    "    featurizer.init_feature_mapping()\n",
    "    #featurizer.update_ystats(trainqs)\n",
    "   \n",
    "\n",
    "    # if feat_onlyseen_preds:\n",
    "    # just do it always\n",
    "    featurizer.update_seen_preds(trainqs)\n",
    "    \n",
    "    return featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734edcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to start training the models\n",
    "trainqs = load_qdata(train_qfns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testqs = load_qdata(test_qfns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a69fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 30\n",
    "lr=0.0001\n",
    "training_opt = \"none\"\n",
    "opt_lr = 0.1\n",
    "swa_start = 5\n",
    "mask_unseen_subplans = 0\n",
    "subplan_level_outputs=0\n",
    "normalize_flow_loss = 1\n",
    "heuristic_unseen_preds = 0\n",
    "cost_model = \"C\"\n",
    "use_wandb = 0\n",
    "eval_fns = \"qerr,plancost\"\n",
    "load_padded_mscn_feats = 1\n",
    "mb_size = 1024\n",
    "weight_decay = 0.0\n",
    "load_query_together = 0\n",
    "result_dir = \"./results\"\n",
    "\n",
    "onehot_dropout=0\n",
    "onehot_mask_truep=0.8\n",
    "onehot_reg=0\n",
    "onehot_reg_decay=0.1\n",
    "eval_epoch = 200\n",
    "optimizer_name=\"adamw\"\n",
    "clip_gradient=20.0\n",
    "loss_func_name = \"mse\"\n",
    "hidden_layer_size = 128\n",
    "num_hidden_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cardinality_estimation.mscn import MSCN as MSCN2\n",
    "from cardinality_estimation.mscn import MSCNCaptum as MSCN2\n",
    "\n",
    "featurizer = init_featurizer(\"set\")\n",
    "\n",
    "mscn = MSCN2(max_epochs = max_epochs, lr=lr,\n",
    "                training_opt = training_opt,\n",
    "                test_random_bitmap = 0,\n",
    "                inp_dropout = 0.0,\n",
    "                hl_dropout = 0.0,\n",
    "                comb_dropout = 0.0,\n",
    "                max_num_tables = -1,\n",
    "                opt_lr = opt_lr,\n",
    "                swa_start = swa_start,\n",
    "                mask_unseen_subplans = mask_unseen_subplans,\n",
    "                subplan_level_outputs=subplan_level_outputs,\n",
    "                normalize_flow_loss = normalize_flow_loss,\n",
    "                heuristic_unseen_preds = heuristic_unseen_preds,\n",
    "                cost_model = cost_model,\n",
    "                use_wandb = use_wandb,\n",
    "                eval_fns = eval_fns,\n",
    "                load_padded_mscn_feats = load_padded_mscn_feats,\n",
    "                mb_size = mb_size,\n",
    "                weight_decay = weight_decay,\n",
    "                load_query_together = load_query_together,\n",
    "                result_dir = result_dir,\n",
    "                onehot_dropout=onehot_dropout,\n",
    "                onehot_mask_truep=onehot_mask_truep,\n",
    "                onehot_reg=onehot_reg,\n",
    "                onehot_reg_decay=onehot_reg_decay,\n",
    "                # num_hidden_layers=num_hidden_layers,\n",
    "                eval_epoch = eval_epoch,\n",
    "                optimizer_name=optimizer_name,\n",
    "                clip_gradient=clip_gradient,\n",
    "                loss_func_name = loss_func_name,\n",
    "                hidden_layer_size = hidden_layer_size,\n",
    "                other_hid_units = hidden_layer_size,\n",
    "                num_hidden_layers = 2,\n",
    "                early_stopping = False,\n",
    "                random_bitmap_idx = False,\n",
    "                reg_loss = False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mscn.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate model\n",
    "eval_alg(mscn, EVAL_FNS, trainqs, \"train\")\n",
    "\n",
    "#preds = mscn.test(testqs)\n",
    "eval_alg(mscn, EVAL_FNS, testqs, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_alg(mscn, EVAL_FNS, testqs, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62089a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = \"results/\" + mscn.get_exp_name() + \"/\"\n",
    "print(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from captum library\n",
    "from captum.attr import LayerConductance, LayerActivation, LayerIntegratedGradients\n",
    "from captum.attr import IntegratedGradients, DeepLift, GradientShap, NoiseTunnel, FeatureAblation, Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#featurizer = init_featurizer(\"set\")\n",
    "ds = QueryDataset(trainqs[0:10], featurizer,\n",
    "        True,\n",
    "        load_padded_mscn_feats=True)\n",
    "loader = data.DataLoader(ds,\n",
    "        batch_size=1, shuffle=False,\n",
    "        collate_fn=mscn_collate_fn_together,\n",
    "        )\n",
    "\n",
    "testds = QueryDataset(testqs[0:3], featurizer,\n",
    "        True,\n",
    "        load_padded_mscn_feats=True)\n",
    "\n",
    "testloader = data.DataLoader(testds,\n",
    "        batch_size=1, shuffle=False,\n",
    "        collate_fn=mscn_collate_fn_together,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9893dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "iloader = iter(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deffab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatch,y,info = next(iloader)\n",
    "print(torch.sum(xbatch[\"pred\"][:,:,20]))\n",
    "print(torch.sum(xbatch[\"pred\"][:,:,21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c518b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlparse\n",
    "sql = testqs[0][\"sql\"]\n",
    "print(sqlparse.format(sql, reindent=True, keyword_case='upper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3335996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(featurizer.featurizer_type_idxs[\"col_onehot\"])\n",
    "print(featurizer.columns_onehot_idx)\n",
    "# mii.info\n",
    "# n.name\n",
    "# n.name_pcode_cf\n",
    "# n.surname_pcode\n",
    "#[0  1  6  8  9 11 17 18 20 21 34 37 38 39 40 41 42 43 44 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b090526",
   "metadata": {},
   "outputs": [],
   "source": [
    "curx = xbatch[\"pred\"].detach().numpy()\n",
    "print(curx.shape)\n",
    "xsum = curx.sum(axis=0).sum(axis=0)\n",
    "\n",
    "print(xsum.shape)\n",
    "print(xsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbb73a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_vecs(curx, curattrs):\n",
    "    #idxs = 0\n",
    "    xsum = curx.sum(axis=0).sum(axis=0)\n",
    "    zero_idxs = xsum == 0\n",
    "    curattrs = curattrs[:,:,~zero_idxs]\n",
    "    idxs = np.array(range(len(xsum)))[~zero_idxs]\n",
    "    \n",
    "    #print(curx)\n",
    "    #print(zero_idxs)\n",
    "    #print(idxs)\n",
    "    \n",
    "    curx = curx[:,:,~zero_idxs]\n",
    "    \n",
    "    assert curx.shape == curattrs.shape\n",
    "    \n",
    "    # TODO: avg based on non-zero elements\n",
    "    \n",
    "    #print(\"attr sum: \", np.sum(curattrs))\n",
    "    curattrs = np.abs(curattrs)\n",
    "    #print(\"attr sum after abs: \", np.sum(curattrs))\n",
    "    \n",
    "    attr_sum = curattrs.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "    assert attr_sum.shape[0] == curx.shape[-1]\n",
    "    \n",
    "    # TODO: do we need this?\n",
    "    #attr_sum = attr_sum / np.linalg.norm(attr_sum, ord=1)\n",
    "    \n",
    "    # TODO: do this or no?\n",
    "    curx_nonz = curx != 0\n",
    "    \n",
    "    xnonzero_sums = curx_nonz.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "    #TODO?\n",
    "    attr_sum = attr_sum / xnonzero_sums\n",
    "    \n",
    "    return idxs, attr_sum\n",
    "\n",
    "def get_mscn_attrs(xbatch, attrs, featurizer, normalize=True):\n",
    "    '''\n",
    "    returns a vector with x-axis names and attribution values;\n",
    "    '''\n",
    "    batchsize = xbatch[\"table\"].shape[0]\n",
    "    assert batchsize == attrs[0].shape[0]\n",
    "    tabidxs, tabattrs = get_attr_vecs(xbatch[\"table\"].detach().numpy(), attrs[0].detach().numpy())\n",
    "    predidxs, predattrs = get_attr_vecs(xbatch[\"pred\"].detach().numpy(), attrs[1].detach().numpy())\n",
    "    print(predidxs)\n",
    "    joinidxs, joinattrs = get_attr_vecs(xbatch[\"join\"].detach().numpy(), attrs[2].detach().numpy())\n",
    "    \n",
    "    # TODO: need to do sample_bitmaps\n",
    "    tablabels = []\n",
    "    for curtabidx in tabidxs:\n",
    "        for tab,tidx in featurizer.table_featurizer.items():\n",
    "            if tidx == curtabidx:\n",
    "                tablabels.append(tab)\n",
    "                break\n",
    "    joinlabels = []\n",
    "    for curjidx in joinidxs:\n",
    "        for join,jidx in featurizer.join_featurizer.items():\n",
    "            found = False\n",
    "            if curjidx == jidx:\n",
    "                joinlabels.append(join)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            joinlabels.append(\"unknown\")\n",
    "    # TODO: join-stats\n",
    "    \n",
    "    predlabels = []\n",
    "    colstart,collen = featurizer.featurizer_type_idxs[\"col_onehot\"]\n",
    "    # TODO: if stats used\n",
    "    #colstatsstart,colstatsend = self.featurizer_type_idxs[\"col_stats\"]\n",
    "    cmp_start,cmplen = featurizer.featurizer_type_idxs[\"cmp_op\"]\n",
    "    cstart,clen = featurizer.featurizer_type_idxs[\"constant_continuous\"]\n",
    "    lstart,llen = featurizer.featurizer_type_idxs[\"constant_like\"]\n",
    "    dstart,dlen = featurizer.featurizer_type_idxs[\"constant_discrete\"]\n",
    "    hstart,hlen = featurizer.featurizer_type_idxs[\"heuristic_ests\"]\n",
    "    \n",
    "    #print(hstart, hlen)\n",
    "    for pi in predidxs:\n",
    "        if pi >= colstart and pi < colstart+collen:\n",
    "            found = False\n",
    "            for col,colidx in featurizer.columns_onehot_idx.items():\n",
    "                if colidx == pi-colstart:\n",
    "                #if colidx == pi:\n",
    "                    print(col)\n",
    "                    predlabels.append(col)\n",
    "                    found = True\n",
    "                    break     \n",
    "            if not found:\n",
    "                print(pi)\n",
    "                predlabels.append(\"col-unknown\")\n",
    "        elif pi >= cmp_start and pi < cmp_start+cmplen:\n",
    "            predlabels.append(\"cmp\")\n",
    "        elif pi == cstart:\n",
    "            #predlabels.append(\"<\")\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi == cstart+1:\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi >= lstart and pi < lstart+llen:\n",
    "            #predlabels.append(\"Like-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Like-Hashes\")\n",
    "        elif pi >= dstart and pi < dstart+dlen:\n",
    "            #predlabels.append(\"Constant-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Constant-Hashes\")\n",
    "        elif pi == hstart:\n",
    "            predlabels.append(\"PostgreSQL Est (Table)\")\n",
    "        elif pi == hstart+1:\n",
    "            predlabels.append(\"PostgreSQL Est (Subplan)\")\n",
    "    \n",
    "    assert len(predidxs) == len(predlabels)\n",
    "#     print(len(predidxs), len(predlabels))\n",
    "#     print(predidxs)\n",
    "#     print(predlabels)\n",
    "    attrs = np.concatenate([tabattrs, joinattrs, predattrs])\n",
    "    xlabels = tablabels + joinlabels + predlabels\n",
    "    \n",
    "    if normalize:\n",
    "        attrs = attrs / np.linalg.norm(attrs, ord=1)\n",
    "    \n",
    "    return xlabels,attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fe157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardinality_estimation.nets import *\n",
    "\n",
    "def get_model_attrs(mscn_model, weights, xbatch):\n",
    "    n_out = 1\n",
    "    sfeats = mscn_model.sample_mlp1.in_features\n",
    "    pfeats = mscn_model.predicate_mlp1.in_features\n",
    "    jfeats = mscn_model.join_mlp1.in_features\n",
    "\n",
    "    net = SetConvNoFlow(sfeats,\n",
    "        pfeats, jfeats,\n",
    "        128,\n",
    "        n_out=n_out,\n",
    "        dropouts=[0.0, 0.0, 0.0])\n",
    "    net.load_state_dict(weights)\n",
    "    \n",
    "    model = net\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "    #ig_nt = NoiseTunnel(ig)\n",
    "    #dl = DeepLift(model)\n",
    "    #gs = GradientShap(model)\n",
    "    #fa = FeatureAblation(model)\n",
    "    #limea = Lime(model)\n",
    "    ig_attr_test = ig.attribute(tuple([xbatch[\"table\"], xbatch[\"pred\"],\n",
    "                            xbatch[\"join\"], \n",
    "                            xbatch[\"tmask\"], xbatch[\"pmask\"], \n",
    "                                   xbatch[\"jmask\"]]), n_steps=50)\n",
    "    #print(\"ig done\")\n",
    "    \n",
    "    xlabels, igattrs = get_mscn_attrs(xbatch, ig_attr_test, featurizer, normalize=False)\n",
    "\n",
    "    return xlabels, igattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01413c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e29c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = mscn.net.state_dict()\n",
    "xlabels,attrs = get_model_attrs(mscn.net, weights, xbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba44dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mscn.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c943b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_attrs(xlabels, attrs, ax=None):\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        ax = plt.axes()\n",
    "\n",
    "    #plt.yticks(fontsize=20)\n",
    "    sns.barplot(x=attrs, y=xlabels, color='#4260f5', orient=\"horizontal\", ax=ax, ci=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# if onehot_dropout:\n",
    "#     with open(\"dropout-attrs.pkl\", \"wb\") as f:\n",
    "#         pickle.dump([xlabels, attrs], f)\n",
    "# else:\n",
    "#     with open(\"default-attrs.pkl\", \"wb\") as f:\n",
    "#         pickle.dump([xlabels, attrs], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attrs(xlabels, attrs)\n",
    "plt.yticks(fontsize=20)\n",
    "#plt.show()\n",
    "plt.savefig(\"attribution-dropout.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e0bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer.featurizer_type_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xbatch[\"table\"].sum(axis=[0,1]))\n",
    "print(xbatch[\"pred\"].sum(axis=[0,1]))\n",
    "print(xbatch[\"join\"].sum(axis=[0,1]))\n",
    "\n",
    "print(xbatch[\"table\"].shape)\n",
    "print(xbatch[\"pred\"].shape)\n",
    "print(xbatch[\"join\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53809304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_vecs_single(curx, curattrs):\n",
    "    #idxs = 0\n",
    "    #print(curx.shape)\n",
    "    #print(curattrs.shape)\n",
    "    xsum = curx.sum(axis=0).sum(axis=0)\n",
    "    #print(xsum.shape)\n",
    "    zero_idxs = xsum == 0\n",
    "    curattrs = curattrs[:,:,~zero_idxs]\n",
    "    idxs = np.array(range(len(xsum)))[~zero_idxs]\n",
    "    \n",
    "    curx = curx[:,:,~zero_idxs]\n",
    "    \n",
    "    assert curx.shape == curattrs.shape\n",
    "    \n",
    "    # TODO: abs values or also accept negative correlations?\n",
    "    curattrs = np.abs(curattrs)\n",
    "    #print(curattrs)\n",
    "    \n",
    "    attr_sum = curattrs.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "    assert attr_sum.shape[0] == curx.shape[-1]\n",
    "    \n",
    "    # TODO: do we need this?\n",
    "    #attr_sum = attr_sum / np.linalg.norm(attr_sum, ord=1)\n",
    "    \n",
    "    # TODO: do this or no?\n",
    "    curx_nonz = curx != 0\n",
    "    \n",
    "    xnonzero_sums = curx_nonz.sum(axis=0).sum(axis=0)\n",
    "    \n",
    "#     print(attr_sum)\n",
    "#     print(xnonzero_sums)\n",
    "    # Do this because different features have different number of copies in the same set, \n",
    "    # e.g, subplan features are in every vector\n",
    "    attr_sum = attr_sum / xnonzero_sums\n",
    "    \n",
    "    return idxs, attr_sum\n",
    "\n",
    "def get_mscn_attrs_single(xbatch, xi, attrs, featurizer, normalize=True):\n",
    "    '''\n",
    "    returns a vector with x-axis names and attribution values;\n",
    "    '''\n",
    "    #batchsize = xbatch[\"table\"].shape[0]\n",
    "    #assert batchsize == attrs[0].shape[0]\n",
    "    tabidxs, tabattrs = get_attr_vecs_single(xbatch[\"table\"][xi:xi+1].detach().numpy(), \n",
    "                                      attrs[0].detach().numpy())\n",
    "    predidxs, predattrs = get_attr_vecs_single(xbatch[\"pred\"][xi:xi+1].detach().numpy(), \n",
    "                                        attrs[1].detach().numpy())\n",
    "    #print(predidxs)\n",
    "    joinidxs, joinattrs = get_attr_vecs_single(xbatch[\"join\"][xi:xi+1].detach().numpy(), \n",
    "                                        attrs[2].detach().numpy())\n",
    "    \n",
    "    # TODO: need to do sample_bitmaps\n",
    "    tablabels = []\n",
    "    for curtabidx in tabidxs:\n",
    "        for tab,tidx in featurizer.table_featurizer.items():\n",
    "            if tidx == curtabidx:\n",
    "                tablabels.append(tab)\n",
    "                break\n",
    "    \n",
    "    joinlabels = []\n",
    "    for curjidx in joinidxs:\n",
    "        for join,jidx in featurizer.join_featurizer.items():\n",
    "            found = False\n",
    "            if curjidx == jidx:\n",
    "                joinlabels.append(join)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            joinlabels.append(\"unknown\")\n",
    "    # TODO: join-stats\n",
    "    \n",
    "    predlabels = []\n",
    "    colstart,collen = featurizer.featurizer_type_idxs[\"col_onehot\"]\n",
    "    # TODO: if stats used\n",
    "    #colstatsstart,colstatsend = self.featurizer_type_idxs[\"col_stats\"]\n",
    "    cmp_start,cmplen = featurizer.featurizer_type_idxs[\"cmp_op\"]\n",
    "    cstart,clen = featurizer.featurizer_type_idxs[\"constant_continuous\"]\n",
    "    lstart,llen = featurizer.featurizer_type_idxs[\"constant_like\"]\n",
    "    dstart,dlen = featurizer.featurizer_type_idxs[\"constant_discrete\"]\n",
    "    hstart,hlen = featurizer.featurizer_type_idxs[\"heuristic_ests\"]\n",
    "    \n",
    "    #print(hstart, hlen)\n",
    "    for pi in predidxs:\n",
    "        if pi >= colstart and pi < colstart+collen:\n",
    "            found = False\n",
    "            for col,colidx in featurizer.columns_onehot_idx.items():\n",
    "                if colidx == pi-colstart:\n",
    "                #if colidx == pi:\n",
    "                    #print(col)\n",
    "                    predlabels.append(col)\n",
    "                    found = True\n",
    "                    break     \n",
    "            if not found:\n",
    "                #print(pi)\n",
    "                predlabels.append(\"col-unknown\")\n",
    "                \n",
    "        elif pi >= cmp_start and pi < cmp_start+cmplen:\n",
    "            predlabels.append(\"cmp\")\n",
    "        elif pi == cstart:\n",
    "            #predlabels.append(\"<\")\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi == cstart+1:\n",
    "            predlabels.append(\"range-filter\")\n",
    "        elif pi >= lstart and pi < lstart+llen:\n",
    "            #predlabels.append(\"Like-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Like-Hashes\")\n",
    "        elif pi >= dstart and pi < dstart+dlen:\n",
    "            #predlabels.append(\"Constant-Hash-\" + str(pi))\n",
    "            predlabels.append(\"Constant-Hashes\")\n",
    "        elif pi == hstart:\n",
    "            predlabels.append(\"PostgreSQL Est (Table)\")\n",
    "        elif pi == hstart+1:\n",
    "            predlabels.append(\"PostgreSQL Est (Subplan)\")\n",
    "    \n",
    "    assert len(predidxs) == len(predlabels)\n",
    "\n",
    "    attrs = np.concatenate([tabattrs, joinattrs, predattrs])\n",
    "    xlabels = tablabels + joinlabels + predlabels\n",
    "    \n",
    "    if normalize:\n",
    "        #attrs = attrs / np.linalg.norm(attrs, ord=2)\n",
    "        attrs = attrs / np.sum(attrs)\n",
    "        #print(np.sum(attrs))\n",
    "    \n",
    "    return xlabels,attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597563ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 1\n",
    "sfeats = mscn.net.sample_mlp1.in_features\n",
    "pfeats = mscn.net.predicate_mlp1.in_features\n",
    "jfeats = mscn.net.join_mlp1.in_features\n",
    "    \n",
    "net = SetConvNoFlow(sfeats,\n",
    "    pfeats, jfeats,\n",
    "    128,\n",
    "    n_out=1,\n",
    "    dropouts=[0.0, 0.0, 0.0])\n",
    "net.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7971e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "qrep = testqs[0]\n",
    "subsetg = qrep[\"subset_graph\"]\n",
    "node_list = list(subsetg.nodes())\n",
    "node_list.sort()\n",
    "\n",
    "sfeats = mscn.net.sample_mlp1.in_features\n",
    "pfeats = mscn.net.predicate_mlp1.in_features\n",
    "jfeats = mscn.net.join_mlp1.in_features\n",
    "\n",
    "\n",
    "model = net\n",
    "model.eval()\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "assert xbatch[\"table\"].shape[0] == len(node_list)\n",
    "\n",
    "allsqls = []\n",
    "allxlabels = []\n",
    "alligattrs = []\n",
    "\n",
    "for xi in range(xbatch[\"table\"].shape[0]):\n",
    "    subjg = qrep[\"join_graph\"].subgraph(node_list[xi])\n",
    "    subsql = nx_graph_to_query(subjg)\n",
    "    #print(subsql)\n",
    "    \n",
    "    ig_attr_test = ig.attribute(tuple([xbatch[\"table\"][xi:xi+1], xbatch[\"pred\"][xi:xi+1],\n",
    "                            xbatch[\"join\"][xi:xi+1], \n",
    "                            xbatch[\"tmask\"][xi:xi+1], xbatch[\"pmask\"][xi:xi+1], \n",
    "                                   xbatch[\"jmask\"][xi:xi+1]]), n_steps=50)\n",
    "\n",
    "    #print(\"ig done\")\n",
    "    xlabels, igattrs = get_mscn_attrs_single(xbatch, xi, \n",
    "                                ig_attr_test, featurizer, \n",
    "                                             normalize=True)\n",
    "    #print(\"Xlabels: \", xlabels)\n",
    "    #print(igattrs)\n",
    "    #break\n",
    "    allsqls.append(subsql)\n",
    "    allxlabels.append(xlabels)\n",
    "    alligattrs.append(igattrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving attribute importance file; can be visualized further in the captum jupyter notebook\n",
    "import pickle\n",
    "if onehot_dropout:\n",
    "    with open(\"subplan-dropout-attrs.pkl\", \"wb\") as f:\n",
    "        pickle.dump([allsqls, allxlabels, alligattrs], f, protocol=3)\n",
    "else:\n",
    "    print(\"saving default\")\n",
    "    with open(\"subplan-default-attrs.pkl\", \"wb\") as f:\n",
    "        pickle.dump([allsqls, allxlabels, alligattrs], f, protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8a538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
