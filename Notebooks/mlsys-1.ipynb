{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cardinality_estimation.featurizer import Featurizer\n",
    "from query_representation.query import load_qrep\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-separate",
   "metadata": {},
   "source": [
    "# Setup file paths / Download query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "import errno\n",
    "def make_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "TRAINDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-train\")\n",
    "VALDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-val\")\n",
    "TESTDIR = os.path.join(os.path.join(\"\", \"queries\"), \"mlsys1-test\")\n",
    "\n",
    "RESULTDIR = os.path.join(\"\", \"results\")\n",
    "make_dir(RESULTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-collaboration",
   "metadata": {},
   "source": [
    "# Query loading helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_qdata(fns):\n",
    "    qreps = []\n",
    "    for qfn in fns:\n",
    "        qrep = load_qrep(qfn)\n",
    "        # TODO: can do checks like no queries with zero cardinalities etc.\n",
    "        qreps.append(qrep)\n",
    "        template_name = os.path.basename(os.path.dirname(qfn))\n",
    "        qrep[\"name\"] = os.path.basename(qfn)\n",
    "        qrep[\"template_name\"] = template_name\n",
    "    return qreps\n",
    "\n",
    "def get_query_fns(basedir, template_fraction=1.0):\n",
    "    fns = []\n",
    "    tmpnames = list(glob.glob(os.path.join(basedir, \"*\")))\n",
    "    assert template_fraction <= 1.0\n",
    "    \n",
    "    for qi,qdir in enumerate(tmpnames):\n",
    "        if os.path.isfile(qdir):\n",
    "            continue\n",
    "        template_name = os.path.basename(qdir)\n",
    "        # let's first select all the qfns we are going to load\n",
    "        qfns = list(glob.glob(os.path.join(qdir, \"*.pkl\")))\n",
    "        qfns.sort()\n",
    "        num_samples = max(int(len(qfns)*template_fraction), 1)\n",
    "        random.seed(1234)\n",
    "        qfns = random.sample(qfns, num_samples)\n",
    "        fns += qfns\n",
    "    return fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arranged-waterproof",
   "metadata": {},
   "source": [
    "# Evaluation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(alg, qreps):\n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    ests = alg.test(qreps)\n",
    "    return ests\n",
    "\n",
    "def eval_alg(alg, eval_funcs, qreps, samples_type, result_dir=\"./results/\"):\n",
    "    '''\n",
    "    '''\n",
    "    np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "    alg_name = alg.__str__()\n",
    "    exp_name = alg.get_exp_name()\n",
    "    \n",
    "    if isinstance(qreps[0], str):\n",
    "        # only file paths sent\n",
    "        qreps = load_qdata(qreps)\n",
    "    \n",
    "    ests = alg.test(qreps)\n",
    "\n",
    "    for efunc in eval_funcs:\n",
    "        rdir = None\n",
    "        if result_dir is not None:\n",
    "            rdir = os.path.join(result_dir, exp_name)\n",
    "            make_dir(rdir)\n",
    "\n",
    "        errors = efunc.eval(qreps, ests, samples_type=samples_type,\n",
    "                result_dir=rdir,\n",
    "                num_processes = -1,\n",
    "                alg_name = alg_name)\n",
    "\n",
    "        print(\"{}, {}, #samples: {}, {}: mean: {}, median: {}, 99p: {}\"\\\n",
    "                .format(samples_type, alg, len(errors),\n",
    "                    efunc.__str__(),\n",
    "                    np.round(np.mean(errors),3),\n",
    "                    np.round(np.median(errors),3),\n",
    "                    np.round(np.percentile(errors,99),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lucky-omaha",
   "metadata": {},
   "source": [
    "# Load queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set template_fraction <= 1.0 to test quickly w/ smaller datasets\n",
    "train_qfns = get_query_fns(TRAINDIR, template_fraction = 0.1)\n",
    "val_qfns = get_query_fns(VALDIR, template_fraction = 1.0)\n",
    "test_qfns = get_query_fns(TESTDIR, template_fraction = 1.0)\n",
    "#trainqs = load_qdata(train_qfns)\n",
    "## can take up a lot of memory, so we avoid loading query data if not needed.\n",
    "#valqs = load_qdata(val_qfns)\n",
    "#testqs = load_qdata(test_qfns)\n",
    "\n",
    "print(\"Selected {} training queries, {} validation queries, {} test queries\".\\\n",
    "      format(len(train_qfns), len(val_qfns), len(test_qfns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-mentor",
   "metadata": {},
   "source": [
    "# What is cardinality estimation and query optimization?\n",
    "\n",
    "We are going to use a simple / small query for visualizing / illustrating the key properties of our tasks. These visualization functions work for larger queries, but we'll need to play around with matplotlib to make them look fine; it gets too large to visualize properly for most CEB queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ad911",
   "metadata": {},
   "source": [
    "### Simple example query\n",
    "\n",
    "* Queries in CEB are stored in *.pkl file because it contains more than just the SQL; Among other things, each query contains cardinality (size) estimates for all <it> subplans </it> of the query. These subplans are the intermediate results encountered by an optimizer when joining and optimizing the query (shown later).\n",
    "* The loaded file is a dictionary; Next we access the SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "qfn = os.path.join(os.path.join(\"queries\", \"example_queries\"), \"2.pkl\")\n",
    "qrep = load_qdata([qfn])[0]\n",
    "print(qrep[\"name\"])\n",
    "print(qrep[\"sql\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e58263",
   "metadata": {},
   "source": [
    "### Join Graph, Cardinality Estimation and Query Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892438d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from query_representation.viz import draw_plan_graph\n",
    "#matplotlib.use('Agg') # no UI backendz\n",
    "\n",
    "join_graph = qrep[\"join_graph\"]\n",
    "nx.draw(join_graph, pos=nx.spring_layout(join_graph), \n",
    "        with_labels=True, node_size=2000, font_size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae73bc",
   "metadata": {},
   "source": [
    "* The two key properties of a query we care about are the join graph, and each of its subplans --- which is any connected subgraph of the join graph.\n",
    "\n",
    "* Note: each node in the join graph represents a table used in the SQL query above (the node label is the ALIAS used in the SQL, such as title as t); An edge in the join graph implies there is a condition such as t.id = ci.movie_id in the query above.\n",
    "\n",
    "* The process of optimizing a query is essentially like contracting the join graph to a point of single node; at each step, you are allowed to combine (``join``) any two connected nodes. Any order of applying the joins is semantically an equivalent way to execute a SQL query. The job of an optimizer is to choose a fast way to execute the queries. More [details](https://bertwagner.com/posts/does-the-join-order-of-my-tables-matter/)\n",
    "\n",
    "* The join graph does not show the query' filters / predicates. For instance, `rt.role IN ('actor', 'director')` in the SQL query above. IN CEB, we will have a few different join graphs, and then for each join graph, A LOT of queries in which we vary these filters --- which changes the sizes of each node in the join graph; These filters can also interact with each other after joins --- and how these sizes change is crucial for choosing between different orders of executing the joins.\n",
    "\n",
    "* Cardinality estimation is about estimating sizes for all the subplans (i.e., connected subgraphs of the join graph). These subgraphs would be encountered by an optimizer when it considers a particular sequence of joins. Processing a large sized join can be expensive --- because that will involve more CPU, more memory usage, and so on; \n",
    "\n",
    "* Some join orders can be particularly bad. For instance, consider joining the tables `cast_info (ci)` and `title (t)` first in the query above. These are both very large tables; Joining them, creates an even larger intermediate results. Future join operations will have to process these intermediate rows. Instead, if you join `cast_info` with `role_type`, then as the filter predicate: `rt.role IN ('actor', 'director')` reduces the size of the `cast_info` table in a non-trivial way. And so on. \n",
    "\n",
    "* Note: If the filter on title (t.production_year <= 2015) was different, such as `t.production_year <= 1890`, which will mean there would be very few qualifying rows in the `title` table, then joining the `cast_info` and `title` tables in the first step may have been a good move!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b1b64",
   "metadata": {},
   "source": [
    "### Subset Graph / Plan Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying, because we will do changes to it for visualizing, such as adding a source node.\n",
    "from cardinality_estimation.algs import TrueCardinalities\n",
    "import copy\n",
    "\n",
    "subsetg = copy.deepcopy(qrep[\"subset_graph\"])\n",
    "subsetg = subsetg.reverse()\n",
    "\n",
    "nodes = list(subsetg.nodes())\n",
    "nodes.sort(key=lambda x: len(x))\n",
    "final_node = nodes[-1]\n",
    "\n",
    "true_alg = TrueCardinalities()\n",
    "y = true_alg.test([qrep])[0]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(15,10))\n",
    "SOURCE_NODE = tuple([\"s\",])\n",
    "\n",
    "draw_plan_graph(subsetg, y, \"C\", ax=ax, source_node=SOURCE_NODE,\n",
    "               final_node=final_node, font_size=20, width=.0, edge_color=None, \n",
    "               bold_opt_path=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4a7e4",
   "metadata": {},
   "source": [
    "* It is useful to represent all these subplans in a graph structure of its own. This lets us reason about query optimization in terms of standard graph theoretic constructs. In code, we refer to this as the subset_graph; Intuitively, it should be clear what each node and edges of this graph represent.\n",
    "\n",
    "* Please see Section 4 of this [paper](https://vldb.org/pvldb/vol14/p2019-negi.pdf) for precise mathematical definitions of what we show here --- including how edges are defined etc. , and note that we referred to it as the plan graph in the paper. Another recent [interesting paper](https://arxiv.org/abs/2105.08878) referred to the same structure as cardinality estimation graphs.\n",
    "\n",
    "* Each edge in this subset graph is a join in the query. An optimizer typically costs each join using various heuristics. We use a simple, but practically reasonable, cost model (referred to as `C` in the [paper](https://vldb.org/pvldb/vol14/p2019-negi.pdf)) --- these are shown in green-yellow-red colors going from cheaper to expensive operations (note: the costs are a function of cardinalities; we use true cardinalities to cost these edges). Each path from the top `S` node, to the bottom-most node represents a [left deep plan]\n",
    "(https://www.mathcs.emory.edu/~cheung/Courses/554/Syllabus/5-query-opt/left-deep-trees.html) in the context of query optimization. Thus, the shortest path from the top node (S) to the bottom node (D) is equivalent to an optimizer finding the cheapest (left deep) plan (typically, using dynamic programming algorithms). The shortest path given the true cardinalities is highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dfec29",
   "metadata": {},
   "source": [
    "### The task\n",
    "\n",
    "* The task is to give a size estimate to each node in the subset graph shown above. Each node corresponds to a particular subgraph of the join graph, i.e., a particular sequence of joins. Thus, there is a corresponding SQL query, and a true label for this query (which we have generated by executing these queries). \n",
    "\n",
    "* The learning task is to take a representation of the SQL query associated with each node (featurizes the tables, joins, and filters in the query), and output a size estimate for it.\n",
    "\n",
    "* For instance, we will show how we evaluate two baselines below --- the actual true cardinalities, and the estimates from PostgreSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242b7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardinality_estimation.algs import TrueCardinalities, Postgres\n",
    "true_alg = TrueCardinalities()\n",
    "pg_alg = Postgres()\n",
    "y = true_alg.test([qrep])[0]\n",
    "ypg = pg_alg.test([qrep])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6139371",
   "metadata": {},
   "source": [
    "### Evaluation 1: Q-Error\n",
    "\n",
    "* Q-Error = `max((y/yhat), (yhat/y))`\n",
    "* This was defined by Moerkotte et al. in the [paper](http://www.vldb.org/pvldb/vol2/vldb09-657.pdf). It provides strong arguments for why this is the right way to evaluate cardinality estimates if we care about query plans (as opposed to error metrics like mean squared error). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f164336",
   "metadata": {},
   "source": [
    "### Evaluation 2: Plan-Cost\n",
    "\n",
    "* Here, we will take the estimates provided, cost each edge in the subset graph based on those estimates, and find the best plan based on these estimated costs. \n",
    "\n",
    "* As we see below, the plan chosen using PostgreSQL estimates was different from the plan chosen using true estimates. \n",
    "\n",
    "* The `true` cost of this plan (the cost of the edges using the true cardinalities) would be the plan-cost. Intuitively, this cost is a good proxy for how good / bad the plan is, or how long it will take to execute them (as long as the cost model we use reasonably represents the execution scenario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(15,10))\n",
    "#from query_representation.utils import SOURCE_NODE\n",
    "SOURCE_NODE = tuple([\"s\",])\n",
    "draw_plan_graph(subsetg, ypg, \"C\", ax=ax, source_node=SOURCE_NODE,\n",
    "               final_node=final_node, font_size=20, width=.0, edge_color=None, \n",
    "               bold_opt_path=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-yesterday",
   "metadata": {},
   "source": [
    "# Evaluation Functions\n",
    "\n",
    "* Now, that we have explained what these evaluation functions are, here we just import their implementations to be used for evaluating the learned models.\n",
    "* SimplePlanCost is an approximation to a DBMS based plan cost (for instance, PostgresPlanCost --- implemented in evaluation.eval_fns as well). There are two simplifying assumptions in SimplePlanCost:\n",
    "    * Only considers left deep plans\n",
    "    * Cost model is quite simple simple; it only really makes sense in case of index nested loop joins. In reality, indices may not exist, other join kinds may be better etc. Even so, it is fairly correlated with Postgres Plan Cost in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval_fns import QError, SimplePlanCost\n",
    "EVAL_FNS = []\n",
    "EVAL_FNS.append(QError())\n",
    "EVAL_FNS.append(SimplePlanCost())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-chapter",
   "metadata": {},
   "source": [
    "# Helper function for initializing featurizer\n",
    "\n",
    "* Featurizer object contains information about the db, e.g., tables, joins, columns, how to featurize predicate filters etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_featurizer(featurization_type):\n",
    "    # Load database specific data, e.g., information about columns, tables etc.\n",
    "    dbdata_fn = os.path.join(TRAINDIR, \"dbdata.json\")\n",
    "    featurizer = Featurizer(None, None, None, None, None)\n",
    "    with open(dbdata_fn, \"r\") as f:\n",
    "        dbdata = json.load(f)\n",
    "    featurizer.update_using_saved_stats(dbdata)\n",
    "\n",
    "    # ynormalization: takes log(y) for all target values, y.\n",
    "    featurizer.setup(ynormalization=\"log\",\n",
    "            featurization_type=featurization_type)\n",
    "    featurizer.update_ystats(trainqs)\n",
    "    return featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734edcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to start training the models\n",
    "trainqs = load_qdata(train_qfns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-equipment",
   "metadata": {},
   "source": [
    "# Submission code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_submission(predictions, filename):\n",
    "    \"Take model output & save for cardinality estimation benchmark upload.\"\"\"\n",
    "    np.save(filename, np.array([value for item in predictions for key, value in sorted(item.items())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-ranking",
   "metadata": {},
   "source": [
    "# RandomForest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardinality_estimation.algs import RandomForest\n",
    "featurizer = init_featurizer(\"combined\")\n",
    "rf = RandomForest(grid_search = False,\n",
    "                n_estimators = 10,\n",
    "                max_depth = 6)\n",
    "rf.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "eval_alg(rf, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(rf, EVAL_FNS, val_qfns, \"val\")\n",
    "\n",
    "preds = get_preds(rf, test_qfns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-springer",
   "metadata": {},
   "source": [
    "# Save submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_submission(preds, \"mysubmisson.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-ranking",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardinality_estimation.algs import XGBoost\n",
    "featurizer = init_featurizer(\"combined\")\n",
    "xgb = XGBoost(grid_search=False, tree_method=\"hist\",\n",
    "                       subsample=1.0, n_estimators = 100,\n",
    "                       max_depth=10, lr = 0.01)\n",
    "xgb.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "eval_alg(xgb, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(xgb, EVAL_FNS, valqs, \"val\")\n",
    "\n",
    "\n",
    "# TODO: test set prdictions; should submit these for the leaderboard?\n",
    "#preds = xgb.test(testqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-excess",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network / Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardinality_estimation.fcnn import FCNN\n",
    "featurizer = init_featurizer(\"combined\")\n",
    "fcnn = FCNN(max_epochs = 10,\n",
    "     lr=0.0001,\n",
    "     mb_size = 512,\n",
    "     weight_decay = 0.0,\n",
    "     result_dir = \"./results\",\n",
    "     num_hidden_layers=4,\n",
    "     optimizer_name=\"adamw\",\n",
    "     clip_gradient=20.0,\n",
    "     loss_func_name = \"mse\",\n",
    "     hidden_layer_size = 256)\n",
    "\n",
    "fcnn.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "eval_alg(fcnn, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(fcnn, EVAL_FNS, valqs, \"val\")\n",
    "\n",
    "# TODO: test set prdictions; should submit these for the leaderboard?\n",
    "#preds = fcnn.test(testqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-execution",
   "metadata": {},
   "source": [
    "# Multi Set Convolutional Network\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Introduced by Kipf et al. in this [paper](https://arxiv.org/abs/1809.00677). Architecture based on [Deep Sets](https://arxiv.org/abs/1703.06114).\n",
    "* Does not reserve an exact mapping for features on a particular table / column. Treats table features, join features, and predicate features as set of vectors. Has practical benefits over the flat 1d featurization (see discussion in README). But requires each batch to have same shape; thus a lot of the smaller query features need to be padded with zeros, which makes the memory consumption become much larger (can probably improve this somehow).\n",
    "* load_padded_mscn_feats = True (see MSCN initialization below), loads these padded sets in memory; takes more RAM, but is faster; load_padded_mscn_feats = False, pads the vectors as needed --- takes longer to train (TODO: current python implementation can be improved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cardinality_estimation.mscn import MSCN\n",
    "\n",
    "featurizer = init_featurizer(\"set\")\n",
    "\n",
    "# load_padded_mscn_feats = True means all the fea\n",
    "mscn = MSCN(max_epochs = 10,\n",
    "     load_padded_mscn_feats = False,\n",
    "     lr=0.0001,\n",
    "     mb_size = 512,\n",
    "     weight_decay = 0.0,\n",
    "     result_dir = \"./results\",\n",
    "     optimizer_name=\"adamw\",\n",
    "     clip_gradient=20.0,\n",
    "     loss_func_name = \"mse\",\n",
    "     hidden_layer_size = 256)\n",
    "\n",
    "mscn.train(trainqs, valqs=None, testqs=None,\n",
    "    featurizer=featurizer, result_dir=RESULTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "eval_alg(mscn, EVAL_FNS, trainqs, \"train\")\n",
    "eval_alg(mscn, EVAL_FNS, valqs, \"val\")\n",
    "\n",
    "# TODO: test set prdictions; should submit these for the leaderboard?\n",
    "#preds = mscn.test(testqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-harvard",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
